# 3장
## 1. 선형회귀
- 선형 회귀란 독립 변수 올 사용해 종속 변수 y의 움직 임을 예측하고 설명하는 작업
- 하나의 x 값만으로도 y 값을 설명가능-> 단순 선형 회귀
- x 값이 여러 개 필요-> 다중 선형 회귀
- 임의의 직선을 그어 이에 대한 평균 제곱 오차를 구하고, 이 값을 가장 작게 만들어 주는 a와 b 값을 찾아가는 작업
## 2. 최소제곱법
- 일차 함수의 기울기 a와 절편 b를 바로 구할 수 있음
- 주어진 x의 값이 하나일 때 적용 가능
-  최소 제곱법 공식

![image](https://github.com/sejongsmarcle/2024_Winter_Ai_study/assets/128311918/7c4ea811-8af2-4d6e-8dba-a5dffc005eae)
 ![image](https://github.com/sejongsmarcle/2024_Winter_Ai_study/assets/128311918/f85de03d-c746-45c8-b657-9422bd9b9184)
## 3. 최소제곱 코딩
1. 넘파이 라이브러리를 불러온 후 앞에서 나온 데이터 값을 리스트 형식으로 x와 y로 정의　　

![image](https://github.com/sejongsmarcle/2024_Winter_Ai_study/assets/128311918/d29c53c9-2275-4686-a289-30aa85f44d2a)  
2．mx 라는 변수에 x원소들의 평균값을，my에 y 원소들의 평균값을 입력  
+  mean(): x의 모든 원소의 평균을 구하는 넘파이 함수  
   ![image](https://github.com/sejongsmarcle/2024_Winter_Ai_study/assets/128311918/323f719d-cb94-415f-8742-a06c6b5c075a)  
3. x의 각 원소와 ：x의 평균값들의 차를 제곱하라는 명령
   ![image](https://github.com/sejongsmarcle/2024_Winter_Ai_study/assets/128311918/ce34f5b3-e67d-40fc-bd84-f8b02139829a)  
4.  새로운 함수를 정의하여 변수에 분자의 값을 저장한 뒤 임의의 변수 선의 초깃값을 0으로 설정하고 x의 개수만큼 실행
    선에 x의 원소와 평균의 차, y의 각 원소와 평균의 차를 곱해서 차례로 더해줌
   ![image](https://github.com/sejongsmarcle/2024_Winter_Ai_study/assets/128311918/b28f89f8-7c34-4c57-a943-c85bb8072b13)
5. a 값과 b 값 도출  
   ![image](https://github.com/sejongsmarcle/2024_Winter_Ai_study/assets/128311918/48cc932d-f141-4bf3-a6a7-08ca63ff475b)![image](https://github.com/sejongsmarcle/2024_Winter_Ai_study/assets/128311918/7eff01ad-ea6b-43f1-bf6b-56b17af076bd)
## 4. 평균제곱 오차
- 입력값이 여러개인 경우 오류가 발생 가능
- 이를 해결하기 위해 여러 개의 입력값을 계산할 때에는 임의의 선을 그은 후 이 선이 얼마나 잘 그려졌는지를 평가하고 이를 조금씩 수정함
- 선의 오차를 평가하기 위해서는 오차 평가 알고리즘이 필요하면 그 중에서 평균제곱오차(MSE)가 가장 많이 활용됨
- 오차를 구하는 방정식  
![image](https://github.com/sejongsmarcle/2024_Winter_Ai_study/assets/128311918/ef9363a0-0e11-4368-9f0a-248de60d3a25)  
- 오차의 합을 구하는 방정식  
![image](https://github.com/sejongsmarcle/2024_Winter_Ai_study/assets/128311918/81d8d548-9493-4e46-985a-6d0ca14f2638)  
- 평균 제곱 오차  
 ![image](https://github.com/sejongsmarcle/2024_Winter_Ai_study/assets/128311918/fc58fe33-178a-48fd-bf5a-4c1dd98dd15c)  
--- 
# 4장
## 1. 경사하강법
- 그래프에서 오차를 비교하여 가장 작은 방향으로 이동시키는 방법
- 미분 기울기를 이용하며 반복적으로 기울기 a를 변화시켜서 m의 값을 찾아내는 방법
## 2. 학습률
- 기울기의 부호를 바꿔 이동시킬 때 적절한 거리를 찾지 못해 너무 멀리 이동시키면 a 값이 한
  점으로 모이지 않기에 어느만큼 이동시킬지 신중한 결정이 필요함.
  이때 이동거리를 정해주는 것을 학습률이라고 함.
  - 딥러닝에서 학습률의 값을 적절히 바꾸면서 최적의 학습률을 찾는 것은 중요한 최적화 과정 중 하나
## 3. 다중 선형 회귀
- 변수의 개수가 여러 개일 때 사용
- 다중 선형 회귀의 경우 1차원 예측 직선이 3차원 예측 평면으로 변경.
-  과외 수업 횟수라는 새로운 변수가 추가되면서 1차원 직선에서만 움직이던 예측 결과가 더 넓은 평면 범위 안에서 움직이게 되며, 이로 인해 더 정밀한 예측이 가능해짐. 
