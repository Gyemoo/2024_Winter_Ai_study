# 3장. 가장 훌륭한 예측선 긋기: 선형 회귀
- 딥러닝의 두 가지 계산 원리: 선형 회귀, 로지스틱 회귀
- 선형 회귀: 가장 훌륭한 예측선 긋기

## 1. 선형 회귀의 정의
- 머신러닝과 딥러닝은 정보를 필요로하며 이를 통해 방정식을 만들 수 있다.
- ‘x 값이 변함에 따라 y 값도 변한다.’ (x를 독립 변수, y를 종속 변수)
- 독립 변수를 사용해 종속 변수의 움직임을 예측하는 것을 선형 회귀라고 한다.
- 하나의 x 값만으로 y 값을 설명하는 것을 단순 선형 회귀, x 값을 여러 개 필요로 하는 것을 다중 선형 회귀

## 2. 가장 훌륭한 예측선이란?
- 독립 변수가 하나뿐인 단순 선형 회귀에 대해서
- y=ax+b(일차 함수 그래프) 이 직선을 그리기 위해서는 직선의 기울기인 a 값과 y의 절편 b 값을 정확히 예측해야 한다. -> a,b 값을 찾아야 함.

## 3. 최소 제곱법
- 일차 함수의 기울기 a와 y절편 b를 구할 수 있는 공식, 최소 제곱법 (변수가 하나일 때)
- a = (x-x 평균)(y-y 평균)의 합 / (x-x 평균)제곱의 합
- b = y의 평균 - (x의 평균 * 기울기 a)
- 완성된 직선의 방정식에서 x를 대입했을 때 나오는 y 값을 ‘예측값’

## 4. 코딩으로 확인하는 최소 제곱
- 모든 원소의 평균 = mean()
- 최소 제곱근 공식 중 분모 -> sum(), **(제곱), for I in x(x의 각 원소를 한 번씩 I 자리에 대입)

## 5. 평균 제곱 오차
- 여러 개의 입력 값을 계산할 경우, 임의의 선을 그리고 평가하며 조금씩 수정해 가는 방법을 사용
- 평균 제곱 오차: 오차를 평가하는 방법

## 6. 잘못 그은 선 바로잡기
- ‘일단 그리고 조금씩 수정해 나가기’ 오차를 계산하여 오차가 작은 쪽으로 바꾸는 알고리즘 필요
- 직선의 오차 확인 법: 각 점과 그래프 사이의 거리를 잰 그 직선들의 합 (합이 작을수록 잘 그어진 직선)
- 오차 = 예측 값 – 실제 값
- 오차의 합을 구할 때는 각 오차의 값을 제곱해 준다.
- 선형 회귀란 임의의 직선을 그어 이에 대한 평균 제곱 오차를 구하고, 이 값을 가장 작게 만들어 주는 a와 b 값을 찾아가는 작업입니다.

## 7. 코딩으로 확인하는 평균 제곱 오차
