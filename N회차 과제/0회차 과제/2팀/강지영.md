# 3장
## 1. 선형 회귀(linear regression): 가장 훌륭한 예측선 긋기

+ 변수 x와 y의 관계를 'x 값이 변함에 따라 y값도 변한다'라고 정의했을 때 x를 독립 변수, y를 종속 변수라 함
+ 단순 선형 회귀: 하나의 x값으로 y값을 설명
+ 다중 선형 회귀: 여러 개의 x값으로 y값을 설명

+ y = ax + b라는 일차함수에서 상수 a와 b를 찾아내는 작업을 선형 회귀라고 함

## 2. 최소 제곱법(method of least squares): 단순 선형 회귀에 적용
<img width="178" alt="스크린샷 2024-01-08 192027" src="https://github.com/sejongsmarcle/2024_Winter_Ai_study/assets/128336150/3b1ec4da-57a3-482c-9f6f-3430e998add5">
<br/>
<img width="189" alt="스크린샷 2024-01-08 192231" src="https://github.com/sejongsmarcle/2024_Winter_Ai_study/assets/128336150/200f36b1-83e5-4377-ac57-de4dc4de6bdd">


```
import numpy as np

# x 값과 y 값 
x=[2, 4, 6, 8]
y=[81, 93, 91, 97]

# x와 y의 평균값
mx = np.mean(x)
my = np.mean(y)
print ("x의 평균값:" , mx)
print("y의 평균값:" , my)

# 기울기 공식의 분모
divisor = sum([(mx - i)* * 2 for i in x])
# 기울기 공식의 분자
def top (x, mx, y, my):
    d = 0
    for i in range (len (x) ) ：
        d += ( x [i ] - mx) * ( y [i ] - my)
    return d
dividend = top(x, mx, y, my)

print ("분모:" , divisor)
print ("분자:" , dividend)

# 기울기와 y 절편 구하기
a = dividend / divisor
b = my - (mx*a)

# 출력으로 확인
print("기울기 a =", a)
print("y 절편 b =", b)
```

## 3. 평균 제곱 오차 (mean square error, MSE)

### ‘일단 그리고 조금씩 수정해 나가기’ 
+ 오차 = 예측값 - 실제값 <br/>
<img width="134" alt="image" src="https://github.com/sejongsmarcle/2024_Winter_Ai_study/assets/128336150/e885d5e5-9cc6-4d55-a6b3-0a98af5151da">
<br/>
<img width="200" alt="image" src="https://github.com/sejongsmarcle/2024_Winter_Ai_study/assets/128336150/6b9a3729-5972-4f82-8f56-34d19d398550">
<br/>-> 선형 회귀란 임의의 직선을 그어 이에 대한 평균 제 
곱 오차를 구하고, <br/>이 값을 가장 작게 만들어 주는 a와 b 값을 찾아가는 작업

```
import numpy as np

# 기울기 a와 y 절편 b
fake_a_b = [3, 76]

# x, y 의 데이터 값
data = [ [2 , 81 ], [4, 93】, [6, 91], [8, 97]]
x = [i[0] for i in data]
y = [i[1] for i in data]

# y = ax + b에 a와 b 값을 대입하여 결과를 출력하는 함수 
def predict (x) ：
    return fake_a_b [0]*x + fake_a_b[1]

# MSE 함수
def mse(y, y_hat):
return ( (y-y_hat) ** 2 ) .mean())

# MSE 함수를 각 y 값에 대입하여 최종 값을 구하는 함수 
def mse_val(y, predict_result):
    return mse(np.array(y), np.array(predict_result))

# 예측 값이 들어갈 빈 리스트 
predict_result = []

# 모든 x 값을 한 번씩 대입하여 
for i in range(len (x) ) ：
    # predict_result 리스트를 완성 
    predict_result. append(predict(x[ i ] ))
    print("공부한 시간=%.f, 실제 점수 =%.f, 예측 점수=%.fn % ( x [i ], y [i ],
    predict ( x [i] ) ) )

# 최종 MSE 출력
print("mse 최 종 값 : " + str(mse_val(predict_ result, y)))
```

# 4장
## 1. 경사 하강법: 다중 선형 회귀에서 사용 가능

+ 오차를 수정하기 위해 경사 하강법 사용!
+ 반복적으로 기울기 a를 변화시켜서 접선의 기울기가 0인 지점을 찾아냄

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as pit

# 공부 시간 X와 성적 Y의 리스트를 만들기
data = [[2, 81], [4, 93], [6, 9 1 】, [8, 97]] 
x = [i[0] for i in data] 
y = [i[1] for i in data]

# 그래프로 나타내기
plt .figure(figsize=(8,5)) 
plt.scatter(x, y) 
plt.show( )

# 라스트로 되어 았는 때 y 값을 넘파이 배열로 바꾸기(인덱스를 주어 하나씩 불러와 계산이 가능하게 하기 위함) 
x_data = np.array(x)
y_data = np.array(y)

# 기울기 a와 절편 b의 값 초기화 
a = 0
b = 0

# 학습률 정하기 
lr = 0.03

# 몇 번 반복될지 설정 
epochs = 2001

# 경사 하강법 시작
for i in range(epochs) : # 에포크 수만큼 반복
    y_pred = a * x_data + b #y를 구 하 는 식 세 우 기 
    error = y_data - y_pred # 오차 를 구 하 는 식
    # 오차 함수를 a로 미분한 값
    a_diff = - ( 2 /len( x_ data) ) * sum( x_data * ( error))
    # 오차 함수를 b로 미분한 값
    b_diff = - ( 2 /len( x_ data) ) * sum( error)

    a = a - lr * a_diff # 학습률을 곱해 기존의 a값 업데이트 
    b = b - lr * b.diff # 학습률을 곱해 기존의 b값 업데이트

    if i % 100 = 0 ： # 100번 반복될 때마다 현재의 a값, b값 출력
       print("epo〔h=%.f, 기 울 기 =%.04f, 절 편 =%.04f" % (i, a, b))

# 앞서 구한 기울기와 절편을 이용해 그래프를 다시 그리기 
y_pred = a * x_data + b 
plt.scatter(x, y)
plt.plot([min(x.data), ma x(x_data)]/ [min(y_pred)/ max(y_ pred )]) 
plt.show( )
```

## 2. 학습률(learning rate)

+ 발산하지 않고 적절한 이동 거리를 정해주는 것을 학습률이라고 함
  <br/> ->오차의 변화에 따라 이차 함수 그래프를 만들고 적 
절한 학습률을 설정해 미분 값이 0인 지점을 구하는 것

## 3. 다중 선형 회귀

+ 변수 x가 2개인 경우: y = a1x1+ a2x2 + b

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits import mplot3d

# 공 부 시 간 X와 성적 Y의 리스트 만들기
data = [[2 , 0, 81 ], [4, 4, 93 ], [6, 2, 91], [8, 3, 97]]
x1 = [i[0] for i in data]
x2 = [i[1] for i in data]
y = [i[2] for i in data]

# 그 래 프 로 확인
ax = plt.axes(projection='3d')
ax.set_xlabel('study_hours' )
ax.set_ylabel('private_class')
ax.set_zlabel('Score')
ax.dist = 11
ax.scatter(x1, x2, y)
plt.show( )

# 리스트로 되어 있는 X와 y 값을 넘파이 배열로 바꾸기(인덱스로 하나씩 불 러 와 계산할 수 있도록 하 
기 위함 )
x1_data = np.array(x1)
x2_data = np.array(x2)
y_data = np.array(y)

# 기울기 a와 절편 b의 값 초기화 
a1 = 0
a2 = 0
b = 0

#학습률
lr = 0.02

# 몇 번 반복할지 설정(0부터 세므로 원하는 반복 횟수에 +1)
epochs = 2001

# 경사 하강법 시작
for i in range(epochs) ： # epoch 수 만 큼 반 복
    y_pred = a1 * x1_data + a2 * x2_data + b # 노 를 구 하 는 식 세 우 기 
    error = y_data - y_pred # 오 차 를 구 하 는 식
    # 오 차 함수를 a1 로 미분한 값
    a1_diff = -(2/len (x1 _data)) * sum(x1_data * (error))
    # 오 차 함수를 a2로 미분한 값
    a2_diff = -(2/len (x2 _data)) * sum(x2_data * (error))
    # 오 차 함 수 를 b로 미분한 값
    b_diff = -(2 /len (x1 _data)) * sum(y_data - y_pred)
    a1 = a1 - lr * a1 _diff # 학습률을 곱해 기존의 
    a2 = a2 - lr * a2 _diff # 학습률을 곱해 기존의 a2 값 업데이트 
    b = b - lr * b_diff # 학습를을 곱해 기존의 b값 업데이트

    if i % 100 = 0 : # 100번 반복될 때마다 현재의 a1, a2, b 값 출력
        print("epoch:%.f, 기울기1=%.04f, 기울기2=%.04f, 절편=%.04f"
% ( i , a1, a2, b))
```

<img width="271" alt="스크린샷 2024-01-08 222004" src="https://github.com/sejongsmarcle/2024_Winter_Ai_study/assets/128336150/dca81523-fd31-4a8e-bc45-1375ef273670">

