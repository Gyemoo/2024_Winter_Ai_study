# 선형 회귀란
-> 가장 훌륭한 예측선 긋기
1. 하나의 x 값만으로도 y값을 설명할 수 있을 때 단순 선형 회귀라고 한다.
2. X 값이 여러 개 필요할 때 다중 선형 회귀라고 한다.
3. 선형 회귀는 곧 정확한 직선을 그려내는 과정이다. 즉, 선형 회귀는 결국 최적의 a 값과 b 값을 찾아내는 작업이라고 할 수 있다. (y=ax+b)

# 최소 제곱법
![image](https://github.com/sejongsmarcle/2024_Winter_Ai_study/assets/128350167/b920d6cd-b102-46e9-9709-24d561f6e3b8)
이것이 최소 제곱법 공식이다.

# 코딩으로 확인하는 최소 제곱

```python
import numpy as np
x=[2, 4, 6, 8]
y=[81, 93, 91, 97]
mx = np.mean(x)
my = np.mean(y)
print ("x의 평균값:" , mx)
print ("y의 평균값:" , my)
divisor = sum([(mx - i)* * 2 for i in x])
def top (x, mx, y, my):
  d = 0
  for i in range (len(x)) ：
    d += (x [i]- mx) * (y [i] - my)
  return d
dividend = top(x, mx, y, my)
print ("분모:" , divisor)
print ("분자:" , dividend)
a = dividend / divisor
b = my - (mx*a)

# 출력으로 확인
print("기울기 a =", a)
print("y 절편 b =", b)
```

```python
x의 평균값: 5.0
y의 평균값: 90.5
분모: 20.0
분자: 46.0
기울기 a = 2.3
y 절편 b = 79.0
```

# 평균 제곱 오차 활용하기
잘못 그은 선을 바로잡기 위해서 가장 많이 사용하는 방법은 '일단 그리고 조금씩 수정해 나가기' 방식이다.
### 오차 = 예측 값 - 실제 값
![image](https://github.com/sejongsmarcle/2024_Winter_Ai_study/assets/128350167/e1ebd330-6cb3-428a-8cb4-580234d53d6d)
![image](https://github.com/sejongsmarcle/2024_Winter_Ai_study/assets/128350167/e8450fc2-3462-42f3-9a94-daf0f479b806)
</br>
선형 회귀란 임의의 직선을 그어 이에 대한 평균 제곱 오차를 구하고, 이 값을 가장 작게 만들어 주는 a와 b 값을 찾아가는 작업이다.

# 코딩으로 확인하는 평균 제곱 오차
```python
import numpy as np
fake_a_b = [3, 76]
data = [[2, 81], [4, 93】,[6, 91], [8, 97]]
x = [i[0] for i in data]
y = [i[1] for i in data]
def predict(x)：
  return fake_a_b [0]*x + fake_a_b[1]
def mse(y, y_hat):
  return((y-y_hat)** 2).mean())
def mse_val(y, predict_ r sult):
  return mse(np.array(y), np.array (predict_ result))
predict_result = []
for i in range(len(x))：
  predict_result.append(predict(x[i]))
  print("공부한시간=%.f, 실제 점수 =%.f, 예측 점수=%.f" % (x[i], y[i], predict(x[i])))
print("mse 최종값 : " + str(mse_val(predict_result,y)))
```

```python
공부한 시간=2, 실제 점수=81, 예측 점수=82
공부한 시간=4, 실제 점수=93, 예측 점수=88
공부한 시간=6, 실제 점수=91, 예측 점수=94
공부한 시간=8, 실제 점수=97, 예측 점수=100
```

<br/><br/><br/><br/><br/><br/>
# 경사 하강법
-> 미분 기울기를 이용하는 방법

## 미분 값이 0인 지점 찾는 법
1. a1에서 미분을 구한다.
2. 구해진 기울기의 반대 방향(기울기가 +면 음의 방향, -면 양의 방향으로 얼마 간 이동시킨 a2에서 미분을 구한다.
3. 위에서 구한 미분 값이 0이 아니면 위 과정을 반복한다.

# 학습률
-> 어느 만큼 이동시킬지를 신중히 결정해야 하는데, 이때 이동 거리를 정해 주는 것이다.

# 코딩으로 확인하는 경사하강법
#### 평균 제곱 오차
![image](https://github.com/sejongsmarcle/2024_Winter_Ai_study/assets/128350167/01f2ce39-0d21-4005-84f9-e29535f0d81c)
<br/>
여기서 y=axi + b를 대입하면 <br/>
![image](https://github.com/sejongsmarcle/2024_Winter_Ai_study/assets/128350167/86ba967d-9db1-46f6-bce5-fed318467bbe)

``` python
import numpy as np
import pandas as pd
import matplotlib.pyplot as pit
data = [[2, 81], [4, 93], [6, 91], [8, 97]] 
x = [i[0] fo r i in data] 
y = [i[1] fo r i in data]
pit.figure(figsize=(8,5)) 
pit.scatter(x,y) 
plt.show()
x_data = np.array(x)
y_data = np.array(y)
a = 0
b = 0
lr = 0.03
epochs = 2001
for i in range( epochs): # 에포크 수만큼 반복
  y_pred = a * x_data + b # y를 구하는 식 세우기 
  error = y_data - y_pred # 오차를 구하는 식
  a_diff = -(2/len(x_data)) * sum(x_data *(error))
  b_diff = -(2/len(x_data)) * sum(error)
  a = a-lr * a_diff # 학습률을 곱해 기존의 a값 업데이트 
  b = b-lr * b_diff # 학습률을 곱해 기존의 b값 업데이트
  if i % 100 = 0: # 100번 반복될 때마다 현재의 a값, b값 출력
    print("epoch=%.f, 기울기=%.04f, 절편 =%.04f" % (i, a, b))
y_pred = a * x_data + b 
plt.scatter(x, y)
pi t.plot([min(x.data), max(x_data)], [min(y_pred), max(y_ pred)]) 
plt.show()
```

```python
epoch= 2000, 기울기=23.2000, 절편=79.000 
epoch= 100, 기울기=7.9316, 절편=45.3932
epoch= 200, 기울기=4.7953, 절편=64.109
epoch= 300, 기울기=3.4056, 절편=72.4022 
(중략)
epoch= 1800, 기울기=2.3000, 절편=79.000 
epoch= 1900, 기울기=2.3000, 절편=79.000 
epoch= 2000, 기울기=2.3000, 절편=79.000
```

# 다중 선형 회귀
보다 더 정확한 예측을 하려면 추가 정보를 입력해야 하며, 정보를 추가해 새로운 예측 값을 구하려면 변수의 개수를 늘려 다중 선형 회귀를 만들어 주어야 한다.
<br/>
이는 두 개의 독립 변수 x1과 x2가 생긴 것이다. 이를 사용해 종속 변수 y를 만들 경우 기울기를 두 개 구해야 하므로 다음과 같은 식이 나온다.
<br/><br/>
![image](https://github.com/sejongsmarcle/2024_Winter_Ai_study/assets/128350167/509cd109-3210-411b-bbbb-ec1a9a970ffa)







