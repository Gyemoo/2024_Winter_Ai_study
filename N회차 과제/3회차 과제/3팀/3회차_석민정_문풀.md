### Q - 임시현
1) MSE에서 임의의 직선을 그리고 오차를 구한뒤 (경사하강법)을 이용하여 오차를 수정해가며 (단일 퍼셉트론인 경우) 최적의 a, b값을 찾았다. 이와 유사하게, 신경망(==다층 퍼셉트론) 내부의 가중치는 오차 역전파를 이용하여 수정한다.
2) 또한, XOR 문제를 해결하기 위해서 다층 퍼셉트론의 논리 연산에서는 가중치(W)와 바이어스의 값이 주어진다. 실제 문제에서는 (오차 역전파)를 사용하여 직접 구해야한다.

### Q - 박신영
1) 다층 퍼셉트론과 오차 역전파를 이용하여 만든 신경망을 거듭하여 반복할 경우 고차원적인 사고가 가능한 인공지능을 개발할 수 있을 것만 같은데 그러지 못하는 주된 이유가 되는 문제가 무엇인가?
   -> 기울기 소실 문제
2) 시그모이드 함수의 미분 최댓값은 약 0.3으로, 은닉층을 거듭 내려갈수록 이 미분값이 계속해서 곱해지며 최종적으로는 000의 오차를 수정하는 수정값이 0에 수렴하여 오차 수정이 되지 않는 문제가 생기는데 이때 000는 무엇인가?
   -> 가중치(W)
3) 기울기 소실 문제를 해결하는 방법은?
   -> 활성화 함수를 바꾸는 방법이 있다.
4) 시그모이드 함수를 대체하는 활성화 함수 중 현재 가장 많이 사용되는 함수는 ReLU 함수이다. 시그모이드 함수를 대체할 수 있었던 ReLU 함수의 강점은 무엇인가?
   -> X가 양의 값을 가질 때 기울기가 1이다.

### Q - 김범열
def XOR(x1, x2): return AND(NAND(x1, x2), OR(x1, x2))
