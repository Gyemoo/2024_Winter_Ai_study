1회차_박경준_정리

Chap 5, 6
## **Chap 5**
로지스틱 회귀: 참과 거짓 구분. 적절한 선을 그려가는 과정이다.

시그모이드 함수: S자 형태로 그래프가 그려지는 함수. 여기 역시 ax+b의 값을 구해야 한다.
- a는 그래프의 경사도를 결정한다.
- b는 그래프의 좌우 이동을 의미한다.
- y값이 0과 1 사이라는 특징이 있다.

로그 함수: 시그모이드 함수는 실제 값이 1이나 0일 때, 예측 값이 0이나 1로 가까워지면 오차는 커지는데, 이를 공식으로 만들 수 있게 한다.
- 실제 값이 1일 때는 -logh 그래프를 쓴다.
- 실제 값이 0일 때는 -log(1-h) 그래프를 쓴다.

-{y_data logh + (1-y_data)log(1-h)}

실제 값을 y_data라 할 때, 이 값이 1이면 B<(1-y_data)log(1-h)> 부분이 없어진다.
반대로 0이면 A<y_data logh> 부분이 없어진다.

```python
# 로지스틱 회귀를 위해서는 시그모이드 함수를 사용.
# 0부터 1 사이의 값을 가지는 특성 때문에 로그 함수를 함께 사용.

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 공부 시간 X와 합격 여부 Y 리스트 만들기
data = [[2,0], [4,0], [6,0], [8,1], [10,1], [12,1], [14,1]]
x_data = [i[0] for i in data]	# 공부한 시간 데이터
y_data = [i[1] for i in data]	# 합격 여부

# 그래프로 나타내기
plt.scatter(x_data, y_data)
plt.xlim(0,15)
plt.ylim(-.1,1.1)

a=0; b=0	# 기울기 a와 절편 b의 값 초기화
lr=0.05	# 학습률

# 시그모이드 함수 정의
def sigmoid(x):	# sigmoid라는 이름의 함수 정의
	return 1/(1+np.e**(-x))	# 시그모이드 식의 형태 그대로 파이썬으로 옮김

# 경사 하강법 실행
for i in range(2001):
	for x_data, y_data in data:
		# a에 관한 편미분, 앞서 정의한 sigmoid 함수 사용
		a_diff=x_data*(sigmoid(a*x_data+b)-y_data)
		# b에 관한 편미분
		b_diff=sigmoid(a*x_data+b)-y_data
		# a를 업데이트 하기 위해 a_diff에 학습률 lr을 곱한 값을 a에서 뺌
		a=a-lr*a_diff
		# b를 업데이트 하기 위해 b_diff에 학습률 lr을 곱한 값을 b에서 뺌
		b=b-lr*b_diff
		
		if i%1000==0:
			print("epoch=%.f, 기울기=%.04f, 절편=%.04f" %(i,a,b))

	# 앞서 구한 기울기와 절편을 이용해 그래프 그리기
	plt.scatter(x_data, y_data)
	plt.xlim(0,15)
	plt.ylim(-.1,1.1)
	x_range=(np.arrange(0,15,0.1))	# 그래프로 나타낼 x값의 범위 정하기
	plt.plot(np.arrange(0,15,0.1), np.array([sigmoid(a*x+b)

for x in x_range]))
	plt.show()

# 세 개 이상의 입력 값을 다루게 된다면 시그모이드 함수가 아니라 소프트맥스(softmax)라는 함수를 써야 한다.
```

입력 값을 통해 출력 값을 구하는 함수 y는 다음과 같이 표현할 수 있다.

y=a_1x_1 + a_2x_2 + b

입력 값은 x_1, x_2 // 계산 값은 y // 즉, 출력 값 구하려면 a_1값, a_2값 그리고 b값이 필요하다.

## **Chap 6**
퍼셉트론: 신경망을 이루는 가장 기본 단위 </br>
로지스틱 회귀가 곧 퍼셉트론의 개념이다. </br>

y=ax+b (a는 기울기, b는 y 절편) === y=wx+b (w는 가중치, b는 바이어스) </br>
가중치(weight): w  //  바이어스(bias): b </br>
가중합(weighted sum): 입력값과 가중치의 곱을 모두 더한 다음 거기에 바이어스를 더한 값 </br>
활성화 함수(activation function): 0과 1을 판단하는 함수 (ex: 시그모이드 함수)

AND와 OR 게이트는 직선을 그어 결괏값이 1인 값(검은 점)을 구별할 수 있다. </br>
XOR의 경우 선을 그어 구분할 수 없다.
