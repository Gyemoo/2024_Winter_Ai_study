# 시현님 문제

## 문제1
### binary_crossentropy 손실 함수의 역할은 무엇이며, 왜 시그모이드 활성화 함수와 함께 사용되었을까요?

--> 분류 손실함수 중 하나로 이진 분류 모델에 사용하며, y값(출력값)이 0에서 1 사이의 값인 시그모이드 함수를 사용한다.

## 문제2
### 위에서 사용된 최적화 알고리즘은 'adam'입니다. 'adam'의 특징과 장점에 대해 설명해보세요!

--> Momentum과 RMSProp를 융합한 방법으로 정확도와 보폭의 크기를 개선했다.

## 문제3
### ModelCheckpoint 콜백 함수의 역할에 대해 설명하세요.(인자도 같이 설명해주기!)

--> 특정 지점에서 모델의 가중치를 저장하는 콜백 함수로 filepath는 모델을 저장할 경로, monitor는 모델을 저장할 때 기준이 되는 값

verbose는 1일 경우 모델이 저장 될 때, '저장되었습니다' 라고 화면에 표시되고,0일 경우 화면에 표시되는 것 없이 그냥 바로 모델이 저장된다. 

save_best_only는 True 인 경우, monitor 되고 있는 값을 기준으로 가장 좋은 값으로 모델이 저장된다. False인 경우, 매 epoch마다 모델이 filepath{epoch}으로 저장됩니다. (model0, model1, model2....)

# 예림님 문제

### Q1. 다음 빈칸을 채우시오
#### 조건 : 전체 샘플 중 20%만 사용한다, 케라스 내부에서 테스트 오차와 학습 정확도가 기록된다.
#### y_loss 엔 실험 결과의 오차 값을 저장하고, y_acc엔 정확도의 값을 저장한다.

df = df_pre.sample(frac =  0.2 )

history = model.fit(X, Y, validation_split= 0.33, epochs = 3500, batch_size = 500)

import matplotlib.pyplot as plt

y_yloss = history.histroy['val_loss']

y_acc = history.histroy['acc']

### Q2. 학습이 진행되어도 테스트셋 오차가 줄지 않으면 학습을 멈추게 하는 함수는 ?

--> EarlyStopping()
