## 김민성문제
다음과 같은 코드에서 나올 결과를 적고, 이렇게 나오는 이유를 설명하시오.

```python
from tensorflow.keras.preprocessing.text import Tokenizer
from keras.utils import to_categorical

text= "엔드 게임에서 아이언맨은 죽는다."

token = Tokenizer()
token.fit_on_texts([text])
x = token.texts_to_sequences([text])

word_size = len(token.word_index)+1
x = to_categorical(x, num_classes=word_size)

print(x)
```

- 정답 : 원-핫인코딩을 사용하기 때문 
```python
[[[0. 1. 0. 0. 0.]
  [0. 0. 1. 0. 0.]
  [0. 0. 0. 1. 0.]
  [0. 0. 0. 0. 1.]]]
```


---
## 김태경 문제
```
    padded_x=sequences(x, 4)
    "\n패딩 결과\n", print(padded_x)
```
- 패딩 : 데이터 크기 맞춰줌
```
    token=Tokenizer()
    token.fit_on_texts(docs)
    print(token.word_index)
    x=token.texts_to_sequences(docs)
```
- 토큰화 : 문장을 단어별로 나눠줌
```
    model=Sequential()
    model.add(Embedding(word_size, 8, input_length=4))
    model.add(Flatten())
    model.add(Dense(1, activation='sigmoid'))
    model.compile(optimizer='adam', oss='binary_crossentropy', metrics=['accuracy'])
    model.fit(padded_x, classes, epochs=20)
```
- 단어 임베딩
- 순서 : 2->1->3

