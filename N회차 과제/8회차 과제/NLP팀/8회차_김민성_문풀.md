## 김민성
다음과 같은 코드에서 나올 결과를 적고, 이렇게 나오는 이유를 설명하시오.

```python
from tensorflow.keras.preprocessing.text import Tokenizer
from keras.utils import to_categorical

text= "엔드 게임에서 아이언맨은 죽는다."

token = Tokenizer()
token.fit_on_texts([text])
x = token.texts_to_sequences([text])

word_size = len(token.word_index)+1
x = to_categorical(x, num_classes=word_size)

print(x)
```

## 답
```python
[[[0. 1. 0. 0. 0.]
  [0. 0. 1. 0. 0.]
  [0. 0. 0. 1. 0.]
  [0. 0. 0. 0. 1.]]]
```
이유 : 텍스트 토큰화를 통해서 단어의 빈도 수를 잰 후 단어가 문장의 다른 요소와 어떤 관계를 가지고 있는지를 알아보는 방법인 원-핫 인코딩을 활용하였기 때문이다.
이 때 파이썬 배열의 인덱스는 0부터 시작하고, 케라스에서는 띄어쓰기를 기준으로 하므로 '엔드', '게임에서', '아이언맨은', '죽는다.'로 총 4가지의 단어가 있기에 다음과 같은 결과가 나온다.

<br/><br/><br/>

## 김태경
다음은 교재의 영화 리뷰 긍/부정 판단 NLP의 코드 일부입니다.
코드를 보고, 각 부분의 역할과 1 2 3의 순서를 맞춰주세요.

1.
    padded_x=sequences(x, 4)
    "\n패딩 결과\n", print(padded_x)

2.
    token=Tokenizer()
    token.fit_on_texts(docs)
    print(token.word_index)
    x=token.texts_to_sequences(docs)

3.
    model=Sequential()
    model.add(Embedding(word_size, 8, input_length=4))
    model.add(Flatten())
    model.add(Dense(1, activation='sigmoid'))
    model.compile(optimizer='adam', oss='binary_crossentropy', metrics=['accuracy'])
    model.fit(padded_x, classes, epochs=20)

## 답
1. 패딩(데이터의 크기를 맞춰주는 작업)
2. 토큰화(문장을 단어별로 찢는 작업)
3. 단어 임베딩

순서는 2-1-3 순으로, 토큰화 후 그 데이터의 크기를 패딩을 이용해 맞추고 최종적으로 단어 임베딩을 사용하면 된다.
