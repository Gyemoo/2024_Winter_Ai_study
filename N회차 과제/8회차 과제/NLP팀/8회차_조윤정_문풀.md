# 문제 풀이

## 김민성 문제

```python
from tensorflow.keras.preprocessing.text import Tokenizer
from keras.utils import to_categorical

text= "엔드 게임에서 아이언맨은 죽는다."

token = Tokenizer()
token.fit_on_texts([text])
x = token.texts_to_sequences([text])

word_size = len(token.word_index)+1
x = to_categorical(x, num_classes=word_size)

print(x)
```

결과
```python
[[[0. 1. 0. 0. 0.]
  [0. 0. 1. 0. 0.]
  [0. 0. 0. 1. 0.]
  [0. 0. 0. 0. 1.]]]
```

단어가 문장의 다른 요소와 어떤 관계를 가지고 있는지를 알아보는 방법인 원-핫 인코딩을 활용하였기 때문이다. 이 때 파이썬 배열의 인덱스는 0부터 시작하고, 케라스에서는 띄어쓰기를 기준으로 하는데, '엔드', '게임에서', '아이언맨은', '죽는다.'로 총 4가지의 단어가 있기에 위와 같은 결과가 나왔다!

## 김태경 문제


1. padding

2. tokenizer

3. 임베딩

순서는 2-1-3   
토큰화 후 그 데이터의 크기를 패딩을 이용해 맞추고,    
최종적으로 단어 임베딩을 사용해준다
