#### 4회차_박경준_정리
#### Chap 12, 13 // p.150~179
코드가 많네요..

## **Chap 12**
- 다중 분류(multi classification): 여러 개의 답 중 하나를 고르는 분류 문제
- 이항 분류(binary classification): 둘 중에 하나를 고르는 분류 문제

```python
# 상관도 그래프
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df=pd.read_csv('iris.csv'), names=["sepal_length","sepal_width","petal_length","petal_width","species"])

# 데이터 전체를 한 번에 보는 그래프 출력
sns.pairplot(df,hue='species');	plt.show()
```
```python
# 케라스를 이용해 아이리스의 품종을 예측
from sklearn.preprocessing import LabelEncoder

df=pd.read_csv('iris.csv'), names=["sepal_length","sepal_width","petal_length","petal_width","species"])

e=LabelEncoder()
e.fit(Y_obj)
Y=e.transform(Y_obj)

# 이렇게 하면 array(['Iris-setosa','Iris-versicolor','Iris-virginica'])가
# array([1,2,3])로 바뀐다.

# 활성화 함수를 적용하려면 Y 값이 숫자 0과 1로 이뤄져 있어야 한다.
# 이 조건을 만족시키기 위한 함수
from tensorflow.keras.utils import np_utils
Y_encoded=tf.keras.utils.to_categorical(Y)
# 이렇게 하면 array([1,2,3])가 다시 array([[1.,0.,0.],[0.,1.,0],[0.,0.,1.]])로 바뀐다.
# 여러 개의 Y값을 0과 1로만 이뤄진 형태로 바꿔주는 기법을 원-핫 인코딩이라고 한다.
```
```python
# 소프트맥스
model=Sequential()
model.add(Dense(16,input_dim=4,activation='relu'))
# 최종 출력 값이 3개 중 하나여야 하므로 출력층에 해당하는 Dense의 노드 수를 3으로 설정
model.add(Dense(3,activation='softmax'))
```
- 소프트맥스는 총합이 1인 형태로 바꿔서 계산해 주는 함수.
- 합계가 1인 형태로 변환하면 큰 값이 두드러지게 나타나고 작은 값은 더 작아진다.
```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.preprocessing import LabelEncoder

import pandas as pd
import seaborn as sns
import matplot.pyplot as plt
import numpy as np
import tensorflow as tf

# seed 값 설정
np.random.seed(3)
tf.random.set_seed(3)

# 데이터 입력
df=pd.read_csv('iris.csv'), names=["sepal_length","sepal_width","petal_length","petal_width","species"])

#  그래프로 확인
sns.paairplot("df,hue='species');
plt.show()

# 데이터 분류
dataset=df.values
X=dataset[:,0:4].astype(float)
Y_obj=dataset[:,4]

# 문자열을 숫자로 변환
e=LabelEncoder()
e.fit(Y_obj)
Y=e.transform(Y_obj)
Y_encoded=tf.keras.utils.to_categorical(Y)

# 모델의 설정
model=Sequential()
model.add(Dense(16,input_dim=4,activation='relu'))
model.add(Dense(3,activation='softmax'))

# 모델 컴파일
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# 모델 실행
model.fit(X,Y_encoded,epochs=50,batch_size=1)

# 결과 출력
print("\n Accuracy: %.4f" % (model.evaluate(X,Y_encoded)[1]))
```


## **Chap 13**
- 초음파 광물 예측하기
```python
from keras.models import Sequential
from keras.layers.core import Dense
from sklearn.preprocessing import LabelEncoder

import pandas as pd
import numpy
import tensorflow as tf

# seed 값 설정
numpy.random.seed(3)
tf.random.set_seed(3)

# 데이터 입력
df=pd.read_csv('sonar.csv',header=None)

dataset=df.values
X=dataset[:,0:60]
Y_obj=dataset[:,60]

# 문자열 변환
e=LabelEncoder()
e.fit(Y_obj)
Y=e.transform(Y_obj)

# 모델 설정
model=Sequential()
model.add(Dense(24, input_dim=60, activation=relu'))
model.add(Dense(10, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

# 모델 컴파일
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# 모델 실행
model.fit(X, Y, epochs=200, batch_size=5)

# 결과 출력
print("\n Accuracy: %.4f" % (model.evaluate(X, Y)[1]))

# 이렇게 하면은 정확도는 100%로 나오긴 한다...(?)
```
- 과적합(overfitting): 모델이 학습 데이터셋 안에서는 일정 수준 이상의 예측 정확도를 보이지만, 새로운 데이터에 적용하면 잘 맞지 않는 것을 말한다.
- 과적합을 방지하기 위해서는 학습을 하는 데이터셋과 이를 테스트할 데이터셋을 완전히 구분한 다음 학습과 동시에 테스트를 병행하여 진행하는 것이 한 방법이다.
- 예를 들어, 데이터셋이 총 100개의 샘플이면, 70은 학습셋으로, 30은 테스트셋으로 쓰는 것이다.

- 데이터에 들어있는 모든 샘플을 그대로 테스트에 활용하면 빠른 시간에 모델 서능을 파악하고 수정할 수 있지만, 머싱러닝의 최종 목적은 새로운 데이터를 예측하는 것이기에, 테스트셋을 만들어 정확한 평가를 병행하는 것이 매우 중요하다.
- 학습셋만 갖고 평가할 때는, 층을 더하거나 에포크 값을 높여 실행 횟수를 늘리면 정확도가 계속해서 올라갈 수 있다. 하지만 학습 데이터셋만으로 평가한 예측 성공률이 테스트셋에서도 그대로 나타나지는 않는다.
- 식이 복잡해지고 학습량이 늘어날수록 학습 데이터를 통한 예측률은 계속해서 올라가지만, 테스트셋을 이용한 예측률은 오히려 떨어진다.
```python
# 초음파 광물 예측하기: 학습셋과 테스트셋 구분
from keras.models import Sequential
from keras.layers.core import Dense
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

import pandas as pd
import numpy
import tensorflow as tf

# seed 값 설정
seed=0
numpy.random.seed(seed)
tf.random.set_seed(3)
df=pd.read_csv('sonar.csv',header=None)

dataset=df.values
X=dataset[:,0:60]
Y_obj=dataset[:,60]

e=LabelEncoder()
e.fit(Y_obj)
Y=e.transform(Y_obj)

# 학습셋과 테스트셋의 구분
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=seed)

model=Sequential()
model.add(Dense(24, input_dim=60, activation=relu'))
model.add(Dense(10, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])
model.fit(X_train, Y_train, epochs=130, batch_size=5)

# 테스트셋에 모델 적용
print("\n Accuracy: %.4f" % (model.evaluate(X_test, Y_test)[1]))

# 80.95%의 예측 성공률
```python
# 초음파 광물 예측하기: 모델 저장과 재사용
from keras.models import Sequential, load_model
from keras.layers.core import Dense
from sklearn.preprocessing import LabelEncoder

import pandas as pd
import numpy
import tensorflow as tf

# seed 값 설정
seed=0
numpy.random.seed(seed)
tf.random.set_seed(3)

df=pd.read_csv('sonar.csv',header=None)

dataset=df.values
X=dataset[:,0:60]
Y_obj=dataset[:,60]

e=LabelEncoder()
e.fit(Y_obj)
Y=e.transform(Y_obj)

# 학습셋과 테스트셋의 구분
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=seed)

model=Sequential()
model.add(Dense(24, input_dim=60, activation=relu'))
model.add(Dense(10, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])
model.fit(X_train, Y_train, epochs=130, batch_size=5)
model.save('my_model.h5') 		# 모델을 컴퓨터에 저장

del model		# 테스트를 위해 메모리 내의 모델을 삭제
model=load_model('my_model.h5')		# 모델을 새로 불러옴

# 불러온 모델로 테스트 실행
print("\n Accuracy: %.4f" % (model.evaluate(X_test, Y_test)[1]))
```
- k겹 교차 검증(k-fold cross validation): 실제로 얼마나 잘 작동하는지 확신할 수 없다는 점을 보완해준다.
- 데이터셋을 여러 개로 나눠 하나씩 테스트셋으로 사용하고 나머지를 모두 합해서 학습셋으로 사용하는 방법이다.
- 갖고 있는 데이터의 100%를 테스트셋으로 사용할 수 있다.
```python
# 초음파 광물 예측하기: k겹 교차 검증
from keras.models import Sequential
from keras.layers.core import Dense
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import StratifiedKFold

import pandas as pd
import numpy
import tensorflow as tf

# seed 값 설정
seed=0
numpy.random.seed(seed)
tf.random.set_seed(seed)

df=pd.read_csv('sonar.csv',header=None)

dataset=df.values
X=dataset[:,0:60]
Y_obj=dataset[:,60]

e=LabelEncoder()
e.fit(Y_obj)
Y=e.transform(Y_obj)

# 10개의 파일로 쪼갬
n_fold=10
skf=StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)

# 빈 accuracy 배열
accuracy=[]

# 모델의 설정, 컴파일, 실행
for train, test in skf.split(X,Y):
	model=Sequential()
	odel.add(Dense(24, input_dim=60, activation=relu'))
	odel.add(Dense(10, activation='relu'))
	odel.add(Dense(1, activation='sigmoid'))
	odel.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])
	odel.fit(X[train], Y[train], epochs=100, batch_size=5)
	k_accuracy="%.4f" % (model.evaluate(X[test], Y[test])([1])
	accuracy.append(k_accuracy)

# 결과 출력
print("\n %.4f fold accuracy: " % n_fold, accuracy)
```
