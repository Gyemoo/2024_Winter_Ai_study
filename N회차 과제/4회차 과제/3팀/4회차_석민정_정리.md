## 12) 다중 분류 문제 해결하기
### 다중 분류: 여러 개의 답 중 하나를 고르는 분류 문제로 참(1)과 거짓(0)으로 해결하는 것이 아닌 어떤 것이 답인지를 예측하는 것으로, 둘 중에 하나를 고르는 이항 분류와는 접근 방식이 조금 다름.
+ 오차 함수로는 categorical_crossentropy()를 사용하는 것이 적절함

```
# 상관도 그래프: 속성별로 어떤 연관이 있는지를 보여 줌

sns. pairplot(df, hue='species'); # pairplot(): 데이터 전체를 한번에 보게 해주는 그래프를 출력하는 함수
```

+ 데이터 안에 문자열이 포함되어 있는 경우에는 numpy 보다는 pandas로 데이터를 불러와 X와 Y 값을 구분하는 것이 좋음

```
# sklearn 라이브러리의 LabelEncoder() 함수: 문자열을 숫자로 바꿔주려면 우선 클래스 이름을 숫자 형태로 바꿔 주어야 하기에 사용하는 함수

from sklearn.preprocessing import LabelEncoder

e = LabelEncoder()
e.fit(Y_obj)
Y = e.transform(Y_obj)

# array(['Iris-setosa', 'Iris-versicolor', 'Iris-virgincia']) -> array([1, 2, 3])로 변경되는 것을 확인할 수 있음
```

#### <원-핫-인코딩(one-hot-encoding)>: 여러 개의 Y 값을 0과 1로만 이루어진 형태로 바꿔주는 기법
```
# tf.keras.utils.categorical(): 활성화 함수를 적용하려면 Y 값이 숫자 0과 1로 이루어져 있어야 하기에 사용하는 함수

from.tensorlow.keras.utils import np_utils

Y_encoded = tf.keras.utils.to_categorical(Y)

# array([1, 2, 3]) -> array([[1., 0., 0.], [0., 1., -.], [0., 0., 1.]])로 변경되는 것을 확인할 수 있음
```

#### 소프트맥스(softmax): 총합이 1인 형태로 바꿔서 계산해 주는 활성화 함수로 합계가 1인 형태로 변환하면 큰 값이 두드러지게 나타나고 작은 값은 더 작아짐
+ 입력된 값이 가중합을 거쳐서 소프트맥스 함수를 통해 총합이 1인 형태로 변환되면 이 값들이 교차 엔트로피를 지나 [1., 0., 0.]으로 변하게 되면 우리가 원하는 원-핫 인코딩 값, 즉 하나만 1이고 나머지는 모두 0인 형태로 전환시킬 수 있는 것임


## 13) 과적합 피하기
### 과적합(overfitting): 모델이 학습 데이터셋 안에서는 일정 수준 이상의 예측 정확도를 보이지만, 새로운 데이터에 적용하면 잘 맞지 않는 것을 말함
+ 과적합은 층이 너무 많거나 변수가 복잡해서 발생하기도 하고 테스트셋과 학습셋이 중복될 때 생기기도 함 (딥러닝은 특히 더 주의 필요!)

#### 해결방법- 학습(훈련) 데이터셋과 테스트 데이터셋을 완전히 구분함 다음 동시에 학습과 테스트를 병행하며 진행
+ ex) 100개의 샘플로 이루어진 데이터셋을 70개의 학습셋과 30개의 테스트셋으로 구분하고 신경망을 만들어 학습셋의 학습 결과를 저장함
+ 학습 결과가 저장된 파일을 '모델'이라고 부르며 모델을 다른 셋에 적용할 경우에도 학습 단계에서 각인되었던 그대로 다시 수행됨
+ 모델을 실제 데이터에 적용하여 가장 나은 알고리즘이 만들어지면 이 모델을 실생활에 대입하여 활용하는 것이 바로 머신러닝의 개발 순서임
+ 학습셋만 가지고 평가할때, 층을 더하거나 에포크(epoch) 값을 높여 실행 횟수를 늘리면 정확도가 계속해서 올라갈 수 있지만, 테스트셋에서는 과적합이 발생할 확률이 큼
+ 식이 복잡해지고 학습량이 늘어날수록 학습 데이터를 통한 예측률은 계속해서 올라가지만, 테스트셋을 이용한 예측률은 오히려 떨어짐

```
# sklearn 라이브러리의 train_test_split(): 학습 데이터와 테스트 데이터를 나누어 사용하게 분리해주는 함수

from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=seed) //학습셋과 테스트셋의 구분    

```

 * 더욱 잘 작동하는 모델을 만들기 위해서는 완전히 새로운 데이터셋을 만들어 한번 더 테스트를 진행해주는데 이때 훈련에 쓰이는 데이터셋을 테스트셋이 아닌 검증셋(Validation set) 이라 부르며 마지막에 테스트하기 위해 준비한 데이터셋을 테스트셋이라고 부르게 된다.

```
from keras.models import load_model

model.save('my_model') //학습한 결과를 모델로 저장하기

del model //테스트를 위해 메모리 내의 모델을 삭제하기
model = load_model('my_model') //모델 불러오기
```
#### k겹 교차 검증: 데이터셋을 여러개로 나누어 하나씩 테스트셋으로 사용하고 나머지를 모두 합해서 학습셋으로 사용하는 방법으로 가지고 있는 데이터의 100%를 테스트셋으로 사용할 수 있다는 장점이 있음

```
# sklearn의 StratifiedKFold(): 데이터를 원하는 숫자만큼 쪼개 각각 학습셋과 테스트셋으로 사용되게 만드는 함수

from sklearn.model_selection import StratifiedKFold

n_fold = 10
skf = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)
```

### 최종 코드
```
from keras.model import Sequential
from keras.layers.core import Dense
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import StratifiedKFold

import numpy
import pandas as pd
import tensorflow as tf

#seed 값 설정
seed = 0
numpy.random.seed(seed)
tf.set_random_seed(seed)

df = pd.read_csv('../dataset/sonar.csv', header=None)

dataset = df.values
X = dataset[:,0:60]
Y_obj = dataset[:,60]

e = LabelEncoder()
e.fit(Y_obj)
Y = e.transform(Y_obj)

#10개의 파일로 쪼갬
n_fold = 10
skf = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)

#빈 accuracy 배열
accuracy = []

#모델의 설정, 컴파일 실행
for train, test in skf.split(X, Y):
  model = Sequential()
  model.add(Dense(24, input_dim=60, activation='relu'))
  model.add(Dense(10, activation='relu'))
  model.add(Dense(1, activation='sigmoid'))
  model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])
  model.fit(X[train], Y[train], epochs=100, batch_size=5)
  k_accuracy = "%.4f" % (model.evaluate(X[test], Y[test])[1])
  accuracy.append(k_accuracy)

#결과 출력
print("\n %.f fold accuaracy:" % n_fold, accuracy)
```
