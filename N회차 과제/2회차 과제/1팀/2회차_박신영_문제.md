## 문제 1
다층 퍼셉트론과 오차 역전파를 이용하여 만든 신경망, 


이 신경망을 거듭하여 반복할 경우 고차원적인 사고가 가능한 


인공지능을 개발할 수 있을 것만 같은데 그러지 못한다.


그 주된 이유가 되는 문제는 무엇인가?


답: OOO OO 문제


## 문제 2
문제 1의 정답이 되는 OOO OO 문제는 시그모이드 함수의 미분값과 관련되어있다.


이는 시그모이드 함수의 OOO에 대한 편미분값인데,


시그모이드 함수의 미분 최댓값은 약 0.3으로,


은닉층을 거듭 내려갈수록 이 미분값이 계속 곱해지며


최종적으로는 OOO의 오차를 수정하는 수정값이 0에 수렴하여


오차 수정이 되지 않는 문제가 생긴다.


OOO는 무엇인가?


## 문제 3
문제 1의 정답이 되는 OOO OO 문제를 해결하는 방법은?


## 문제 4
시그모이드 함수를 대체하는 활성화 함수 중


현재 가장 많이 쓰이는 함수는 ReLU 함수이다.


시그모이드를 대체하는, ReLU 함수의 강점은 무엇인가?(객관식)


1. 함수를 만든 사람의 명성이 높다.
2. 함수의 방정식이 간단하다.
3. x가 양의 값을 가질 때 기울기가 일정하다.
4. x가 양의 값을 가질 때 기울기가 1이다.
5. x가 음의 값을 가질 때 y값이 일정하다.




<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>



- 문제 1 답: 기울기 소실 문제(Vanishing Gradient Problem)
- 문제 2 답: 가중치(Weight)
- 문제 3 답: 활성화 함수를 바꾼다.
- 문제 4 답: 4번. x가 양의 값을 가질 때 기울기가 1이다.


문제 4 해설: 함수의 미분값이 1보다 작으면 오차 보정값이 소실된다.


 ReLU 함수의 경우 x>0일 때 미분값이 항상 1이므로 오차 보정값이 소실되지 않는다.
