## 7장
### 1. 다중 퍼셉트론의 설계
![image](https://github.com/sejongsmarcle/2024_Winter_Ai_study/assets/128350167/a7e0a88c-5a93-4db9-be2a-bd4d1fdad874)
![image](https://github.com/sejongsmarcle/2024_Winter_Ai_study/assets/128350167/01de2433-600f-4f5a-8cd9-07ad4a30163f)
<br/>
가운데 숨어있는 은닉층으로 퍼셉트론이 각각 자신의 가중치와 바이어스 값을 보내고，이 은닉층에서 모인 값이 한 번 더 시그모이드 함수)를 이용해 최종 값으로 결과를 보냅니다.
<br/>
![image](https://github.com/sejongsmarcle/2024_Winter_Ai_study/assets/128350167/82e75908-ca71-4b6f-bb65-732417ba084d)
<br/>
위 두 식의 결팟값이 출력층으로 보내집니다. 출력층에서는 역시 시그모이드 함수를 통해 y값이 정해집니다. 이 값을 y(out)이라 할 때 식으로 표현하면 다음과 같습니다.
<br/>
![image](https://github.com/sejongsmarcle/2024_Winter_Ai_study/assets/128350167/d5f05346-cfe8-4b18-b77d-cf38b5f67955)
<br/>
![image](https://github.com/sejongsmarcle/2024_Winter_Ai_study/assets/128350167/53a327e9-8a06-48d6-825e-548bbeaa34ca)
### 2. XOR 문제의 해결
![image](https://github.com/sejongsmarcle/2024_Winter_Ai_study/assets/128350167/bfde465e-8e0d-4792-9600-d152eebf2707)
![image](https://github.com/sejongsmarcle/2024_Winter_Ai_study/assets/128350167/08abb573-d750-4798-a351-0d01707b0aa3)
![image](https://github.com/sejongsmarcle/2024_Winter_Ai_study/assets/128350167/fc39e311-a9b5-424a-8986-412e9baceafa)
### 3. 코딩으로 XOR 문제 해결하기

```python
import numpy as np

# 가중치와 바이어스
w11 = np.array( [-2, -2])
w12 = np.array([2, 2])
w2 = np.array([1, 1]) 
b1 = 3 
b2 = -1 
b3 = -1

# 퍼셉트론
def MLP(x, w, b):
    y = np.sum(w * x) + b 
    if y 〈= 0:
        return 0
    else：
        return 1

# NAND 게이트 
def NAND(x1, x2)：
    return Ml_P(np.array([x1, x2]), w11, b1)

# 0R 게이트 
def 0R(x1, x2):
    return MLP(np.array([x1, x2]), w12, b2)

# AND 게이트 
def AND(x1, x2):
    return MLP(np.array([x1, x2]), w2, b3)

#XOR 게이트 
def X0R(x1, x2)：
    return AND(NAND(x1, x2),0R (x1, x2))

# x1. x2 값을 번갈아 대입하며 최종값 출력 
if__name__ == '__main__ '：
     for x in [(0, 0), (1, 0), (0, 1), (1, 1)]： 
         y = XOR(x[0], x[1])
         print("입력값 : " + str(x) + " 출력 값 : " + str(y))
```

## 8장
### 1. 오차 역전파의 개념
![image](https://github.com/sejongsmarcle/2024_Winter_Ai_study/assets/128350167/9563ebc5-2836-4daf-952c-73b65fc33e64)
![image](https://github.com/sejongsmarcle/2024_Winter_Ai_study/assets/128350167/c5e4eda2-aa2f-4fa7-937b-f565cab61da1)
<br/>
다층 퍼셉트론에서의 최적화 과정을 오차 역전파(back propagation)라고 부릅니다.
### 오차 역전파 구동 방식
1. 임의의 초기 가중치(W)를 준 뒤 결과(y(out))를 계산한다.
2. 계산 결과와 우리가 원하는 값 사이의 오차를 구한다.
3. 경사 하강법을 이용해 바로 앞 가중치를 오차가 작아지는 방향으로 업데이트한다.
4. 위 과정을 더이상 오차가 줄어들지 않을 때까지 반복한다.<br/><br/>
여기서 '오차가 작아지는 방향으로 업데이트한다'는 의미는 미분 값이 0에 가까워지는 방향으로 나아간다는 말입니다. 즉, '기울기가 0이 되는 방향'으로 나아가
야 하는데, 이 말은 가중치에서 기울기를 했을 때 가중치의 변화가 전혀 없는 상태를 말합니다. 따라서 오차 역전파를 다른 방식으로 표현하면 가중치에서 기울
기를 빼도 값의 변화가 없을 때까지 계속해서 가중치 수정 작업을 반복하는 것입니다.<br/>
![image](https://github.com/sejongsmarcle/2024_Winter_Ai_study/assets/128350167/548ddd2c-4411-47a0-a36c-64fe38d18929)

### 2. 코딩으로 확인하는 오차 역전파
![image](https://github.com/sejongsmarcle/2024_Winter_Ai_study/assets/128350167/28b3c2db-7eab-47eb-8fc2-7049c7f5fbae)
1. 환경 변수 지정: 환경 변수에는 입력 값과 타깃 결팟값이 포함된 데이터셋，학습률 등이 포함됩니다. 또한，활성화 함수와 가중치 등도 선언되어야 합니다.
2. 신경망 실행: 초깃값을 입력하여 활성화 함수와 가중치를 거쳐 결괏값이 나오게 합니다.
3. 결과를 실제 값과 비교: 오차를 측정합니다.
4. 역전파 실행: 출력층과 은닉층의 가중치를 수정합니다.
5. 결과 출력

## 9장
### 1. 기울기 소실 문제와 활성화 함수
오차 역전파는 출력층으로부터 하나씩 앞으로 되돌아가며 각 층의 가중치를 수정하는 방법입니다. 그런데 층이 늘어나면서 역전파를 통해 전달되는 이 기울기의 값이 점점
작아져 맨 처음 증까지 전달되지 않는 기울기 소실 문제가 발생합니다.
![image](https://github.com/sejongsmarcle/2024_Winter_Ai_study/assets/128350167/1de61426-cac7-4b29-9cd1-15a97ceb9e45)
<br/>
이는 활성화 함수로 사용된 시그모이드 함수의 특성 때문입니다. 여러 층을 거칠수록 기울기가 사라져 가중치를 수정하기가 어려워지는 것입니다.
<br/>
![image](https://github.com/sejongsmarcle/2024_Winter_Ai_study/assets/128350167/d65919a0-2b09-4763-96fa-d3f1dab7f418)
<br/>
 활성화 함수를 시그모이드가 아닌 여러 함수로 대체하기 시작했습니다.
![image](https://github.com/sejongsmarcle/2024_Winter_Ai_study/assets/128350167/436bd23b-d0b1-4bb6-9ee4-11d93037b4e6)

## 2. 속도와 정확도 문제를 해결하는 고급 경사 하강법
### 확률적 경사 하강법
경사 하강법은 불필요하게 많은 계산량은 속도를 느리게 할 뿐 아니라, 최적 해를 찾기 전에 최적화 과정이 멈출 수도 있습니다. 확률적 경사 하강법은 경사 하강법의 이러한 단점을 보완한 방법입니다.
전체 데이터를 사용하는 것이 아니라. 랜덤하게 추출한 일부 데이터를 사용합니다. 일부 데이터를 사용하므로 더 빨리 그리고 자주 업데이트를 하는 것이 가능해집니다.
![image](https://github.com/sejongsmarcle/2024_Winter_Ai_study/assets/128350167/fd793dbd-5db7-4764-8504-044552869bb0)
### 모멘텀
모멘텀이란 단어는 ‘관성, 탄력, 가속도’라는 뜻입니다. 모멘텀 SGD란 말 그대로 경사 하강법에 탄력을 더해 주는 것입니다. 다시 말해서，경사 하
강법과 마찬가지로 매번 기울기를 구하지만，이를 통해 오차를 수정하기 전 바로 앞 수정 값과 방향(+，-)을 참고하여 같은 방향으로 일정한 비율만 수정되게 하
는 방법입니다. 
![image](https://github.com/sejongsmarcle/2024_Winter_Ai_study/assets/128350167/d4e82f8a-1e45-47c9-9cd1-d0960256f0c5)
<br/>
이밖에 딥러닝 구동에 필요한 고급 경사 하강법과 케라스 내부에서의 활용법을 표로 정리해보면 다음과 같습니다.
<br/>
![image](https://github.com/sejongsmarcle/2024_Winter_Ai_study/assets/128350167/91a058ad-1959-40eb-9d20-22c678bb7c4b)
![image](https://github.com/sejongsmarcle/2024_Winter_Ai_study/assets/128350167/8d9ec150-654e-4bc0-b538-f4bc769f838c)

