## 7장 다층 퍼셉트론
: XOR 문제를 해결하기 위해서는 숨어있는 층인 은닉층(hidden layer)을 만들어 두 개의 퍼셉트론을 한 번에 계산하면 됨.
+ 퍼셉트론이 가중치와 바이어스 값을 은닉층(노드)로 전송->시그모이드 함수를 이용해 최종값으로 결과를 보냄
+ n1; NAND(Negative And) 게이트: x1, x2 모두 1일 때-> 0을 출력 & x1, x2 하나라도 0이 아닐 때-> 1을 출력
+ n2; OR 게이트
+ Y_Out; NAND 게이트와 OR 게이트 값들에 대해 AND 게이트를 수행한 값
+ 신경망: 은닉층을 여러개 쌓아올려 복잡한 문제를 해결함
````
import numpy as np

## 가중치
w11 = np.array([-2, -2])
w12 = np.array([2, 2])
w2 = np.array([1, 1])

## 바이어스
b1 = 3
b2 = -1
b3 = -1

def MLP(x, w, b): ## 퍼셉트론
  y = np.sum(w*x) + b
  if y<=0:
    return 0
  else:
    return 1
  
def NAND(x1, x2): ## NAND 게이트
  return MLP(np.array([x1, x2]), w11, b1)

def OR(x1, x2): ## OR 게이트
  return MLP(np.array([x1, x2]), w12, b2)

def AND(x1, x2): ## AND 게이트
  return MLP(np.array([x1, x2]), w2, b3)

def XOR: ## XOR 게이트
  return AND(NAND(x1, x2), OR(x1, x2))

if __name__ == '__main__': ## x1과 x2값을 번갈아 대입하며 최종값 출력
  for x in [(0, 0), (1, 0), (0, 1), (1, 1)]:
    y = XOR(x[0], x[1])
    printf("입력 값: " + str(x) + " 출력 값: " + str(y))
````

## 8장 오차 역전파
: 다층 퍼셉트론 또한 단일 퍼셉트론의 방식과 같이 결괏값의 오차를 구해 하나 앞선 가중치를 차례로 거슬러 올라가며 조정해 나감
+ 오차 역전파(back propagation): 다층 퍼셉트론에서의 최적화 과정, 가중치에서 기울기를 빼도 값의 변화가 없을 때까지 계속해서 가중치 수정 작업을 반복하는 것
  1) 임의의 초기 가중치(W)를 준 뒤 결과(Y_Out)를 계산함
  2) 계산 결과와 원하는 값 사이의 오차를 구함
  3) 경사 하강법을 이용해 바로 앞 가중치를 오차가 작아지는 방향으로 업데이트함
  4) 위 1)~3) 과정을 더이상 오차가 줄어들지 않을 때(= 미분 값이 0에 가까워지는 방향으로 나아갈 때, 기울기가 0이 되는 방향으로 나아갈 때)까지 반복하여 계산함
** 오차 역전파 코딩 구현 방식
  1) 환경 변수 지정: 환경 변수에는 입력 값과 타깃 결괏값이 포함된 데이터셋, 학습률 등이 포함됨. 또한, 활성화 함수와 가중치 등도 선언되어야 함
  2) 신경망 실행: 초깃값을 입력하여 활성화 함수와 가중치를 거쳐 결괏값이 나오게 함
  3) 결과를 실제 값과 비교: 오차를 측정함
  4) 역전파 실행: 출력층과 은닉층의 가중치를 수정함
  5) 결과 출력
  
## 9장 신경망에서 딥러닝으로
+ 기울기 소실(vanishing gradient) 문제: 층이 늘어나면서 역전파를 통해 전달되는 기울기의 값이 점점 작아져 맨 처음 층까지 전달되지 않음
  -> 여러 층을 거칠수록 기울기가 사라져 가중치를 수정하기가 어려워지는 시그모이드 함수의 특성 때문임
  <하이퍼볼릭 탄젠트(tanh)함수>: 시그모이드 함수의 범위를 -1 -> 1로 확장한 개념, 여전히 1보다 작은 값이 존재하므로 기울기 소실 문제가 해결되지 않음
  <렐루(ReLU)함수>: x가 0보다 작을 땐 모든 값들을 0으로 처리, 0보다 큰 값들은 x를 그대로 사용, x가 0보다 크기만 하면 미분 값이 1이 되므로 여러 은닉층을 거치며 곱해지더라도 맨 처음 층까지 사라지지 않고 남아있을 수 있음
  <소프트플러스(softplus)함수>: 렐루를 변형하여 렐루의 0이 되는 순간을 완화한 함수

+ 다량 계산 문제: 경사 하강법은 한 번 업데이트할 때마다 전체 데이터를 미분해야 하므로 계산량이 많음
  -> 고급 경사 하강법(확률적 경사 하강법, 모멘텀)
  <확률적 경사 하강법(SGD)>: 랜덤하게 추출한 일부 데이터만 사용하므로 더 빠르고 자주 업데이트를 할 수 있음
  <모멘텀(Momentum)>: 오차를 수정하기 전 바로 앞 수정 값과 방향을 참고하여 같은 방향으로 일정한 비율만 수정되게 하는 방법으로 진동과 폭이 줄어드는 효과가 있으며 수정 방향이 지그재그로 일어나는 현상이 줄어들고, 관성의 효과를 내어 경사 하강법에 탄력을 더해 줄 수 있음
  <네스테로프 모멘텀(NAG)>: 모멘텀이 이동시킬 방향으로 미리 이동해서 그레이디언트를 계산하여 불필요한 이동을 줄이는 효과를 나타냄
  <아다그라드(Adagrad)>: 변수의 업데이트가 잦으면 학습률을 적게하여 이동 보폭을 조절하는 방법
  <알엠에스프롭(RMSProp)>: 아다그라드의 보폭 민감도를 보완한 방법
  <아담(Adam)>: 모멘텀과 알엠에스프롭 방법을 합친 바법으로 정확도와 보폭 크기 개선이 가능해짐
  
