#### 2회차_박경준_정리
#### Chap 7, 8, 9

## **Chap 7**
은닉층(hidden layer): 숨어있는 층. 
- XOR 문제를 해결하기 위해 두 개의 퍼셉트론을 한 번에 계산하기 위해 쓰인다.
- 좌표 평면을 왜곡시키는 결과를 가져온다.

노드(node): 은닉층에 모이는 중간 정거장

- 가운데 숨어있는 은닉층으로 퍼셉트론이 각각 자신의 가중치와 바이어스 값을 보내고, 
- 이 은닉층에서 모인 값이 한 번 더 시그모이드 함수를 이용해 최종값으로 결과를 보낸다.

NAND 게이트(Negative-And Gate): n_1값을 잘 보면 입력 값 x_1, x_2가 모두 1일 때 0을 출력하고 하나라도 0이 아니면 1을 출력.
- AND 게이트의 정반대 값을 출력하는 방식이다.

```python
import numpy as np

# 가중치와 바이어스 선언
w11=np.array([-2,-2]);	w12=np.array([2,2])	;w2=np.array([1,1])
b1=3;	b2=-1;	b3=-1

# 퍼셉트론 함수 선언
def MLP(x,w,b):
	y=np.sum(w*x)+b
	if y<=0:
		return 0
	else:
		return 1

# NAND 게이트
def NAND(x1,x2):
	return MLP(np.array([x1,x2]),w11,b1)

# OR 게이트
def OR(x1,x2):
	return MLP(np.array([x1,x2]),w12,b2)

# AND 게이트
def AND(x1,x2):
	return MLP(np.array([x1,x2]),w2,b3)

# XOR 게이트
def XOR(x1,x2):
	return AND(NAND(x1,x2),OR(x1,x2))

# x_1과 x_2값을 번갈아 대입해 가며 최종값 출력
if __name__=='__main__':
	for x in [(0,0),(1,0),(0,1),(1,1)]:
		y=XOR(x[0],x[1])
		print("입력값: " + str(x) + "출력값: " + str(y))
```
은닉층을 여러 개 쌓아올려 복잡한 문제를 해결하는 과정은 뉴런이 복잡한 과정을 거쳐 사고를 낳는 사람의 신경망을 닮았다. </br>
이 방법을 '인공 신경망'이라 부르기 시작했고, 이를 간단히 줄여 '신경망'이라고 통칭한다. </br>

## **Chap 8**
"임의의 가중치를 선언하고 결괏값을 이용해 오차를 구한 뒤 이 오차가 최소인 지점으로 계속해서 조금씩 이동시키는 것!"

경사 하강법은 입출력이 하나일 때, 즉 '단일 퍼셉트론'일 경우.

8장의 경우에는 숨어 있는 층이 하나 더 생겼기 떄문에 계산이 조금 더 복잡해진다.
- 오차 역전파: 다층 퍼셉트론에서의 최적화 과정
- 최적화의 계산 방향이 출력층에서 시작해 앞으로 진행된다.
- 1. 임의 초기 가중치(W)를 준 뒤 결과(y_out)를 계산한다.
- 2. 계산 결과와 우리가 원하는 값 사이의 오차를 구한다.
- 3. 경사 하강법을 이용해 바로 앞 가중치를 오차가 작아지는 방향으로 업데이트한다.
- 4. 위 과정을 더이상 오차가 줄어들지 않을 때까지 반복한다.
- '오차가 작아지는 방향으로 업데이트한다' == '미분 값이 0에 가까워지는 방향으로 나아간다.' 
- == '기울기가 0이 되는 방향으로 나아간다.' == '가중치에서 기울기를 뺐을 때 가중치의 변화가 전혀 없는 상태를 말한다.'

XOR 문제를 오차 역전파 방식으로 해결하기 위해서는 다음과 같은 순서로 구현한다.
- 1. 환경 변수 지정: 환경 변수에는 입력 값과 타깃 결괏값이 포함된 데이터셋, 학습률 등이 포함된다. 또한, 활성화 함수와 가중치 등도 선언돼야 한다.
- 2. 신경망 실행: 초깃값을 입력해 활성화 함수와 가중치를 거쳐 결괏값이 나오게 한다.
- 3. 결과를 실제 값과 비교: 오차를 측정한다.
- 4. 역전파 실행: 출력층과 은닉층의 가중치를 수정한다.
- 5. 결과 출력

## **Chap 9**
층이 늘어나며 역전파를 통해 전달되는 이 기울기의 값이 점점 작아져 맨 처음 층까지 전달되지 않는 기울기 소실 문제가 발생한다.
- 활성화 함수로 사용된 시그모이드 함수의 특성 때문이다.
- 시그모이드 함수를 미분하면 최대치가 0.3으로, 1보다 작기에 계속 곱하다 보면 0에 가까워져서 
- 여러 층을 거칠수록 기울기가 사라져 가중치를 수정하기가 어려워지는 것이다.

하이퍼볼릭 탄젠트(tanh) 함수: 시그모이드 함수의 범위를 01에서 1로 확장한 개념
- 미분한 값의 범위가 함께 확장되는 효과를 가져왔다.
- 여전히 1보다 작은 값이 존재하기에 기울기 소실 문제는 사라지지 않는다.

렐루(ReLU) 함수: 렐루는 x가 0보다 작을 때는 모든 값을 0으로 처리하고, 0보다 큰 값은 x를 그대로 사용한다.

소프트플러스(softplus) 함수: 렐루의 0이 되는 순간을 완화한 변형 함수

경사 하강법은 한 번 업데이트할 때마다 전체 데이터를 미분해야 하기에 계산량이 매우 많다는 단점이 있다.
- 고급 경사 하강법은 이를 보완했다.

- 확률적 경사 하강법: 랜덤하게 추출한 일부 데이터를 사용 --> 더 빠르고 자주 업데이트 가능해짐
- 모멘텀 SGD: 매번 기울기를 구하지만, 오차를 수정하기 전 바로 앞 수정 값과 방향을 참고해 같은 방향으로 일정한 비율만 수정되게 하는 방법
- 아담: 모멘텀과 알엠에스프롭 방법을 합친 방법으로 현재 가장 많이 사용되는 고급 경사 하강법이다.
