# 7장 다층 퍼셉트론
두 개의 퍼셉트론을 한 번에 계산 -> 은닉층(hidden layer) 가 필요\
노드(node) : 은닉층에 모이는 중간 정거장\
n1 = a(x1w11 + x2w21 + b1)\
n2 = a(x1w12 + x2w22 + b2)\
Y(out) = a(n1w31 + n2w32 + b3)\
-> 은닉층을 포함 가중치 6개, 바이어스 3개 필요\
```python
import numpy as np

# 가중치와 바이어스
w11 = np.array([-2, -2])
w12 = np.array([2, 2])
w2 = np.array([1, 1])
b1 = 3
b2 = -1
b3 = -1

# 퍼셉트론
def MLP(x, w, b):
    y = np.sum(w * x) + b
    if y <= 0:
        return 0
    else:
        return 1

# NAND 게이트 
def NAND(x1, x2):
    return MLP(np.array([x1, x2]), w11, b1)

# 0R 게이트 
def 0R(x1, x2):
    return MLP(np.array([x1, x2]), w12, b2)

# AND 게이트 
def AND(x1, x2):
    return MLP(np.array([x1, x2]), w2, b3)

#XOR 게이트
def X0R(x1, x2):
    return AND(NAND(x1, x2),0R (x1, x2))

#x1, x2 값을 번갈아 대입하며 최종값 출력
if __name__ = '__main__':
    for x in [(0 , 0), (1 , 0), (0 , 1), (1 , 1)]:
        y = XOR(x[0], x[1])
        print("입력 값: " + str(x) + " 출력 값: " + str(y))
```
# 8장 오차 역전파
오차 역전파 : 경사 하강법과 다르지 않다\
결과값의 오차를 구해 이를 토대로 하나 앞선 가중치를 차례로 거슬로 올라가며 조정\
W(t + 1) = Wt - a오차/aW\
코딩 공부가 필요해보임;
```python
# 신경망의 실행
class NeuralNetwork:

    # 초깃값 지정
    def __init__(self, num_x, num_yh, num_yo, bias = 1):

        #입력 값(num_x), 은닉층의 초깃값(num_yh), 출력층의 초깃값(num_yo), 바이어스
        self.num_x = num_x + bias # 바이어스는 1로 설정
        self.num_yh = num_yh
        self.num_yo = num_yo

        # 활성화 함수 초깃값
        self.activation_input = [1.0] * self.num_x
        self.activation_hidden = [1.0] * self.num_yh
        self.activation_out = [1.0] * self.num_yo

        # 가중치 입력 초깃값
        self.weight_in = makeMatrix(self.num_x, self.num_yh)

        for i in range(self.num_x):
            for j in range(self.num_yh):
                self.weight_in[i][j] = random.random()

        # 가중치 출력 초깃값
        self.weight_out = makeMatrix(self.num_yh, self.num_yo)
        for j in range(self.num_yh):
            for k in range(self.num_yo):
                self.weight_out[j][k] = random.random()

        # 모멘텀 SGD를 위한 이전 가중치 초깃값
        self.gradient_in = makeMatrix(self.num_x, self.num_yh)
        self.gradient_out = makeMatrix(self.num_yh, self.num_yo)

# 업데이트 함수
def update(self, inputs):

    # 입력층의 활성화 함수
    for i in range(self.num_x - 1):
        self.activation_input[i] = inputs[i]

    # 은닉층의 활성화 함수
    for j in range(self.num_yh):
        sum = 0.0
        for i in range(self.num_x):
            sum = sum + self.activation_input[i] * self.weight_in[i][j]

        # 시그모이드와 tanh중에서 활성화 함수 선택
        self.activation_hidden[j] = tanh(sum, False)

    # 출력층의 활성화 함수
    for k in range(self.num_yo):
        sum = 0.0
        for j in range(self.num_yh):
            sum = sum + self.activation_hidden[j] * self.weight_out[j][k]

        # 시그모이드와 tanh 중에서 활성화 함수 선택
        self.activation_out[k] = tanh(sum, False)

    return self.activation_out[:]

# 역전파 실행
def backPropagate(self, targets):

    # 델타 출력 계산
    output_deltas = [0.0] * self.num_yo
    for k in range(self.num_yo):
        error = targets[k] - self.activation_out[k]
        # 시그모이드와 tanh 중에서 활성화 함수 선택, 미분 적용
        output_deltas[k] = tanh(self.activation_out[k], True) * error

    # 은닉 노드의 오차 함수
    hidden_deltas = [0.0] * self.num_yh
    for j in range(self.num_yh):
        error = 0.0
        for k in range(self.num_yo):
            error = error + output_deltas[k] * self.weight_out[j][k]
            # 시그모이드와 tanh중에서 활성화 함수 선택, 미분 적용
        hidden_deltas[j] = tanh(self.activation_hidden[j], True) * error

    # 출력 가중치 업데이트
    for j in range(self.num_yh):
        for k in range(self.num_yo):
            gradient = output_deltas[k] * self.activation_hidden[j]
            v = mo * self.gradient_out[j][k] - lr * gradient
            self.weight_out[j][k] += v
            self.gradient_out[j][k] = gradient

    # 입력 가중치 업데이트
    for i in range(self.num_x):
        for j in range(self.num_yh):
            gradient = hidden_deltas[j] * self.activation_input[i]
            v = mo * self.gradient_in[i][j] - lr * gradient
            self.weight_in[i][j] += v
            self.gradient_in[i][j] = gradient

    # 오차 계산(최소 제곱법)
    error = 0.0
    for k in range(len(targets)):
        error = error + 0.5 * (targets[k] - self.activation_out[k]) ** 2
    return error

# 학습 실행
def train(self, patterns):
    for i in range(iterations):
        error = 0.0
        for p in patterns:
            inputs = p[0]
            targets = p[1]
            self.update(inputs)
            error = error + self.backPropagate(targets)
    if i % 500 == 0:
        print('error: %-.5f' %error)

# 결과 값 출력
def result(self, patterns):
    for p in patterns:
        print('Input: %s, predict: %s' %(p[0], self.update(p[0])))

if __name__ == '__main__':
    # 두 개의 입력 값, 두개의 레이어, 하나의 출력 값을 갖도록 설정
    n = NeuralNetwork(2, 2, 1)

    # 학습 실행
    n.train(data)

    # 결괏값 출력
    n.result(data)
```
# 9장 신경망에서 딥러닝으로
신경망을 차곡차곡 쌓아올리면 마치 사람처럼 생각하고 판단하는 인공지능이 될것이다? -> Nope
## 1. 기울기 소실 문제와 활성화 함수
기울기 소실: 층이 늘어나면서 역전파를 통해 전달되는 기울기의 값이 점점 작아져 맨 처음 층까지 전달되지 않는다.\
-> 다른 함수를 사용\
1. 시그모이드 함수
2. 하이퍼볼릭 탄젠틓 ㅏㅁ수
3. 렐루 함수
4. 소프트플러스 함수
## 2. 속도와 정확도 문제를 해결하는 고급 경사 하강법
확률적 경사 하강법 : 경사 하강법 보다 빠르고 정확하다
