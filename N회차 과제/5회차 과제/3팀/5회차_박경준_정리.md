#### 5회차_박경준_정리
#### Chap 14(p180~201)

## **Chap 14**
- sample() 함수: 원본 데이터에서 정해진 비율만큼 랜덤으로 뽑아오는 함수
- frac=1이라고 지정하면 원본 데이터의 100%를 불러오라는 의미
- frac=0.5로 지정하면 50%만 랜덤으로 부름

```python
# 와인의 종류 예측하기: 데이터 확인과 실행
from keras.models import Sequential
from keras.layers import Dense
from keras.callbacks import ModelCheckpoint, EarlyStopping

import pandas as pd
import matplotlib.pyplot as plt
import numpy
import tensorflow as tf

# seed값 설정
seed=0
numpy.random.seed(seed)
tf.random.set_seed(3)

# 데이터 입력
df_pre=pd.read_csv('wine.csv', header=None)
df=df_pre.sample(frac=1)
dataset=df.values
X=dataset[:,0:12]
Y=dataset[:,12]

# 모델 설정
model=Sequential()
model.add(Dense(30, input_dim=12, activation='relu'))
model.add(Dense(12, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

# 모델 컴파일
model.compile(loss='binary_crossentropy', optimizer='adam', matrics=['accuracy'])

# 모델 실행
model.fit(X, Y, epochs=200, batch_size=200)

# 결과 출력
print("\n Accuracy: %.4f" % (model.evaluate(X, Y)[1]))
```
```python
# 와인의 종류 예측하기: 데이터 확인과 실행
from keras.models import Sequential
from keras.layers import Dense
from keras.callbacks import ModelCheckpoint	# 모델을 저장하기 위해 케라스의 콜백 함수 중 이 함수를 불러옴

import pandas as pd
import numpy
import os
import tensorflow as tf

# seed값 설정
numpy.random.seed(3)
tf.random.set_seed(3)

# 데이터 입력
df_pre=pd.read_csv('wine.csv', header=None)
df=df_pre.sample(frac=1)
dataset=df.values
X=dataset[:,0:12]
Y=dataset[:,12]

# 모델 설정
model=Sequential()
model.add(Dense(30, input_dim=12, activation='relu'))
model.add(Dense(12, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

# 모델 컴파일
model.compile(loss='binary_crossentropy', optimizer='adam', matrics=['accuracy'])

# 모델 저장 폴더 설정
MODEL_DIR='./model/'	# 모델 저장하는 폴더
if not os.path.exists(MODEL_DIR): os.mkdir(MODEL_DIR)	# 만일 위의 폴더가 존재하지 않으면 이 이름의 폴더를 만들어 줌

# 모델 저장 조건 설정
modelpath="./model/{epoch:02d}-{val_loss:.4f}.hdf5"
checkpointer=ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)

# 모델 실행 및 저장
model.fit(X, Y, validation_split=0.2, epoch=200, batch_size=200, verbose=0, callbacks=[checkpointer])

# val_loss: 테스트 오차는 케라스 내부에서 이와 같이 기록
# acc: 학습 정확도
# val_acc: 테스트셋 정확도
# loss: 학습셋 오차
# 모델이 저장될 곳을 앞서 만든 modelpath로 지정하고 verbose의 값을 1로 정하면 해당 함수의 진행 사항이 출력되고, 0으로 정하면 출력되지 않는다.
```
- 모델의 학습 시간에 따른 정확도와 테스트 결과를 그래프를 통해 확인
- history 변수: 모델이 학습되는 과정을 저장할 곳
- 시간이 너무 오래 걸리지 않게 sample() 함수로 전체 샘플 중 15%만 불러오게 함
- 배치 크기는 500으로 늘려 한 번 딥러닝 가동할 때 더 많이 입력되게끔 함
- 불러온 샘플 중 33%는 분리해 테스트셋으로 사용함
 -y_vloss에 테스트셋(33%)으로 실험한 결과의 오차 값을 저장하고,
- y_acc에 학습셋(67%)으로 측정한 정확도의 값을 저장함.
```python
from keras.models import Sequential
from keras.layers import Dense
from keras.callbacks import ModelCheckpoint

import pandas as pd
import numpy
import os
import matplotlib.pyplot as plt
import tensorflow as tf

# seed값 설정
numpy.random.seed(3)
tf.random.set_seed(3)

# 데이터 입력
df_pre=pd.read_csv('wine.csv', header=None)
df=df_pre.sample(frac=1)
dataset=df.values
X=dataset[:,0:12]
Y=dataset[:,12]

# 모델 설정
model=Sequential()
model.add(Dense(30, input_dim=12, activation='relu'))
model.add(Dense(12, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

# 모델 컴파일
model.compile(loss='binary_crossentropy', optimizer='adam', matrics=['accuracy'])

# 모델 저장 폴더 설정
MODEL_DIR='./model/'
if not os.path.exists(MODEL_DIR): os.mkdir(MODEL_DIR)

# 모델 저장 조건 설정
modelpath="./model/{epoch:02d}-{val_loss:.4f}.hdf5"
checkpointer=ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)

# 모델 실행 및 저장
history=model.fit(X, Y, validation_split=0.33, epochs=3500, batch_size=500)

# y_vloss에 테스트셋으로 실험 결과의 오차 값을 저장
y_vloss=history.history['val_loss']

# y_acc에 학습셋으로 측정한 정확도의 값을 저장
y_acc=history.history['acc']
# x값을 지정하고 정확도를 파란색으로, 오차를 빨간색으로 표시
x_len=numpy.arange(len(y_acc))
plt.plot(x_len, y_vloss, "o", c="red", markersize=3)
plt.plot(x_len, y_acc, "o", c="blue", markersize=3)

plt.show()

# 앞서 공부한 대로 학습셋의 정확도는 시간이 흐를수록 좋아진다.
# 하지만 테스트 결과는 어느 정도 이상 시간이 흐르면 더 나아지지 않는 것을 그래프로 확인할 수 있다.
```
- 학습이 진행될수록 학습셋의 정확도는 올라가지만 과적합 때문에 테스트셋의 실험 결과는 점점 나빠지게 된다.
- 케라스에는 이렇게 학습이 진행돼도 테스트셋 오차가 줄지 않으면 학습을 멈추게 하는 함수가 있다.
```python
form keras.callbacks import EarlyStopping
```
- EarlyStopping() 함수에 모니터할 값과 테스트 오차가 좋아지지 않아도 몇 번까지 기다릴지를 정한다.
```python
early_stopping_callback=EarlyStopping(monitor='val_loss', pattience=100)
model.fit(X, Y, validation_split=0.33, epochs=2000, batch_size=500, callbacks=[early_stopping_callback])
```
