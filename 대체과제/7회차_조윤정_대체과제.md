<10줄 정리>

1. keras는 0~1 사이의 값으로 구성될 때 가장 최적의 성능을 낼 수 있다. 그래서 0~1 사이의 수가 나오도록 정규화를 해준다   

2. 1차원 배열만 주어진다면 위치 정보를 파악하기 어렵다. 그래서 이미지의 특성 또한 파악하기 어렵다. 이 문제를 CNN으로 해결할 수 있다. (그래서 CNN은 이미지/비디오와 같은 고차원 데이터에 효율적이다)   

3. Convolution(합성곱)을 이용한 것이 CNN, 합성곱은 이전에 라플라스 변환과 함께 공부함.
(원본*커널) 값을 이용해서 원본 이미지들의 특징을 추출할 수 있다.   

4. 커널의 종류에 따라 다양한 특징들이 추출되며, CNN은 이 커널의 값들을 학습하는 것이다.
(가중치들이 이 커널을 학습하는 데, 이 과정이 곧 이미지의 특징을 학습하는 것과 같다)   

5. Stride N, 필터를 N칸 씩, 즉 필터를 한 번에 여러 칸 씩 건너 띄어가는 것인데, 한 칸씩 이동하는 것이 아니다 보니 연산량이 감소하는 효과를 볼 수 있다.   

6. Padding, 커널을 씌우면 행렬 차원이 줄어들게 된다. 그런데 이는 곧 정보량이 손실된다는 것을 의미한다. 특히 모서리 부분의 픽셀의 경우 한 번만 사용되어 윤곽 정보가 손실될 수 있다. 차원 축소와 윤곽 자리의 이미지 정보 보존을 위해 경계를 덧대는 전처리 방식을 Padding 이라고 한다. (경계를 0같은 숫자로 덧댄다고 한다, 0으로 덧대면 Zero Padding)   

7. Conv2D는 케라스에서 컨볼루션 층을 추가하는 함수다. 첫 인자 - 커널 개수, kernel_size = (행, 열)-커널 크기 지정, input_shape = (행, 열, 색상(3) 또는 흑백(1)) - 맨 처음층에 입력되는 값을 알려줌   

8. 풀링(서브 심플링)은 컨볼루션 층을 통해 이미지의 특징을 도출한 것을 다시 한 번 축소하는 과정을 의미하며, 정해진 구역에서 최댓값을 뽑아내는 맥스 풀링과 정해진 구역 안에서 평균값을 뽑아내는 평균 풀링이 있다. (model.add(MaxPooling2D(pool_size = 2) #전체 크기가 반으로 줄어듦) 풀링을 이용하면 계산량을 줄이고 이미지의 특징을 강조하는 것이 가능하다
평균 풀링을 이용하면 분산을 사용할 수 있는데, 분산을 이용하면 물체 위치를 보다 쉽게 파악하는 것이 가능하다. 그래서 객체 탐지 분야에서 평균 풀링을 효과적으로 활용할 수 있다.   

9. 드롭아웃은 은닉층에 배치된 노드 중 일부를 꺼주는 것을 의미한다. 플래튼은 2차원 배열을 1차원 배열로 바꾸는 것을 의미한다. 1차원 배열이어야만 활성화 함수 적용이 가능하다   

10. 1차원 배열로 변환해주는 것을 지원해주는 함수는 파이썬의 Numpy에서 raver(), reshape(), flatten()이 있다.   
