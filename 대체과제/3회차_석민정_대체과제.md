<10줄 정리>
+ 단층 퍼셉트론에서는 XOR 문제를 해결할 수 없었지만 다층 퍼셉트론에서는 W와 B 값을 알고 접근하기에 해결이 가능하다.
+ 가중치를 선언하기 위해 넘파이 라이브러리를 불러와 퍼셉트론 함수를 만들어주기 위해 넘파이 합계 함수인 np.sum() 함수를 사용해준다. y값에 따라 0과 1에 해당하는 값을 출력하도록 해준다. 
+ 딥러닝의 기본적인 개사 원리 두 가지는 선형 회기와 로지스틱 회기이다. 이 두 가지는 출력 형식만 다를 뿐 기존에 주어진 정보들을 바탕으로 선을 그어 미래의 값을 예측한다는 점에서 동일하다.
+ 평균 제곱 오차(MSE)는 점들의 특징을 가장 잘 나타내는 a와 b를 찾을 때 사용하여 임의의 선을 그리고 오차를 계산한 후 오차가 작아지는 쪽으로 가도록 한다.
+ 경사 하강법(gradient descent)은 오차 수정과 가중치 수정을 체계적으로 하기 위한 방법으로 가중치에 대한 오차의 변화에 따라 이차 함수 그래프를 만들고 적절한 학습률을 설정해 미분 값이 0인 지점을 구하게 된다.
+ 결과 값의 오차를 구해 이를 토대로 하나 앞선 가중치를 거슬러 올라가며 조정하는 것을 오차 역전파라고 부르며 이는 계산 방향이 출력층에서 앞으로 이동하게 된다. 오차가 작아지는 방향은 기울기가 0이 되는 방향으로 즉, 가중치에서 가중치에 대한 기울기를 빼도 변화가 없음을 의미한다.
+ 경사 하강법은 전체 데이터 셋 사용으로 인한 메모리 부족 가능성과 전체 데이터를 미분해야 하기에 연산량이 증가할 수 있다는 단점이 존재한다.
+ 확률적 경사하강법(SGD: Stochastic Gradient Descent)은 랜덤하게 추출한 일부 데이터를 사용하여 더 빨리 그리고 더 자주 가중치를 업데이트하므로 일반 경사 하강법의 단점을 보완했다는 이점이 있다.
+ 다양한 최적화 알고리즘이 존재하지만, 학습 데이터의 형태에 따라 유효한 전략이 달라질 수 있기에 상황에 맞게 잘 적용하는 능력이 중요할 것이다.
+ 최적화 알고리즘에 대한 깊은 이해는 모델을 구성하고 모델의 성능을 향상하는데 도움을 준다는 것을 알 수 있다.
