### 3회차_박경준_대체과제
#### 1/16 (화) 결석에 관한 대체 과제입니다.

- 다층 퍼셉트론: 앞선 단층 퍼셉트론에서는 XOR 문제가 해결되지 않았으나 다층 퍼셉트론에서는 가능하다.
- 다층 퍼셉트론(신경망) XOR(= NAND and OR)은 오차 역전파를 통해 오차 수정/가중치를 수정할 수 있다.
- 오차가 작아지는 방향은 기울기가 0이 되는 방향, 즉 가중치에서 가중치에 대한 기울기를 뺴도 변화가 없는 방향을 의미한다.
- 단일 퍼셉트론: 오차 수정/가중치 수정 방법으로 경사 하강법을 사용하나, XOR 문제에 있어서 한계가 있다.
- 평균 제곱 오차(MSE)는 점들의 특징을 가장 잘 나타내는 a와 b를 찾을 떄 사용한다. 임의의 선을 그리고 오차를 계산한 후 오차가 작아지는 쪽으로 가도록 하며, 오차 수정/가중치 수정을 체계적으로 하기 위한 방법으로 경사 하강법을 사용한다.
- 경사 하강법(gradient descent)는 오차 수정/가중치 수정 방법이고, 가중치에 대한 오차의 변화에 따라 이차 함수 그래프를 만들고 적절한 학습률을 설정해 미분값이 0인 지점을 구하는 것이다. 최적인 a와 b의 값으 ㄹ구할 수 있다.
- 오차 역전파의 목적은 다층 퍼셉트론에서의 오차 수정이다. 하지만 기울기 소실 문제(은닉층에서 경사하강법 반복 시 오차 수정이 거의 안 되는 상황)이라는 한계점이 존재한다.
- SGD(Stochastic Gradient Descent) 확률적 경사 하강법은 일반 경사 하강법의 단점을 보완해, 랜덤하게 추출한 일부 데이터를 사용해 더 빨리, 자주 가중치를 업데이트한다.
- <문제1> 지그재그가 덜한 상태로 최적의 해를 구하는 방법은 모멘텀으로, 관성의 효과를 적용했기 때문에 수정 방향이 지그재그로 일어나는 현상이 줄어들었다.
- <문제4> 렐루 함수의 한계는 특정 출력이 0이 되면 여태까지 학습하여 곱했던 기울기 값에 0을 곱하게 돼 가중치 업데이트가 안 된다는 것이다. 이는 죽어가는 렐루(Dying ReLU)라고 부른다.
