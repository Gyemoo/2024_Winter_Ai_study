#### 1/16 (화) 결석에 관한 대체 과제입니다.
#### 영상 10줄 정리와 문풀 과제가 모두 포함되어있습니다.
-----
### 3회차 박경준 영상 10줄 정리 대체과제
- 다층 퍼셉트론: 앞선 단층 퍼셉트론에서는 XOR 문제가 해결되지 않았으나 다층 퍼셉트론에서는 가능하다.
- 다층 퍼셉트론(신경망) XOR(= NAND and OR)은 오차 역전파를 통해 오차 수정/가중치를 수정할 수 있다.
- 오차가 작아지는 방향은 기울기가 0이 되는 방향, 즉 가중치에서 가중치에 대한 기울기를 뺴도 변화가 없는 방향을 의미한다.
- 단일 퍼셉트론: 오차 수정/가중치 수정 방법으로 경사 하강법을 사용하나, XOR 문제에 있어서 한계가 있다.
- 평균 제곱 오차(MSE)는 점들의 특징을 가장 잘 나타내는 a와 b를 찾을 떄 사용한다. 임의의 선을 그리고 오차를 계산한 후 오차가 작아지는 쪽으로 가도록 하며, 오차 수정/가중치 수정을 체계적으로 하기 위한 방법으로 경사 하강법을 사용한다.
- 경사 하강법(gradient descent)는 오차 수정/가중치 수정 방법이고, 가중치에 대한 오차의 변화에 따라 이차 함수 그래프를 만들고 적절한 학습률을 설정해 미분값이 0인 지점을 구하는 것이다. 최적인 a와 b의 값으 ㄹ구할 수 있다.
- 오차 역전파의 목적은 다층 퍼셉트론에서의 오차 수정이다. 하지만 기울기 소실 문제(은닉층에서 경사하강법 반복 시 오차 수정이 거의 안 되는 상황)이라는 한계점이 존재한다.
- SGD(Stochastic Gradient Descent) 확률적 경사 하강법은 일반 경사 하강법의 단점을 보완해, 랜덤하게 추출한 일부 데이터를 사용해 더 빨리, 자주 가중치를 업데이트한다.
- <문제1> 지그재그가 덜한 상태로 최적의 해를 구하는 방법은 모멘텀으로, 관성의 효과를 적용했기 때문에 수정 방향이 지그재그로 일어나는 현상이 줄어들었다.
- <문제4> 렐루 함수의 한계는 특정 출력이 0이 되면 여태까지 학습하여 곱했던 기울기 값에 0을 곱하게 돼 가중치 업데이트가 안 된다는 것이다. 이는 죽어가는 렐루(Dying ReLU)라고 부른다.
-----
#### 3회차_박경준_문제풀이 복습_대체과제

<details><summary><임시현 문제 풀이></summary>
<p>

```python
MSE에서 우린 임의의 직선을 그리고 오차를 구한 뒤 "경사 하강법"을 이용하여 오차를 수정해가며
최적의 a와 b값을 찾았다("단일" 퍼셉트론인 경우). 이와 유사하게, 신경망(== "다층" 퍼셉트론)
내부의 가중치는 "오차 역전파"를 이용하여 수정한다.
또한, "XOR" 문제를 해결하기 위해 앞서 사용한 다층 퍼셉트론의 논리 연산에선 "가중치(W)"와 바이어스의 값이 주어졌다.
그런데 실제 문제에선 우리가 직접 구해야 하는데, 이때 사용하는 것이 "오차 역전파"이다.
```
</p>
</details>


<details><summary><정경훈 문제 풀이></summary>
<p>

```python
import numpy as np
import matplotlib.pyplot as plt
from keras.activations import relu

# 데이터 형성하기
x_values = np.linspace(-5, 5, 100)
y_values_keras = relu(x_values)

# matplotlib으로 그리기
plt.plot(x_values, y_values_keras, label='Keras ReLU')
plt.title('ReLU Function')
plt.xlabel('Input')
plt.ylabel('Output')
plt.legend()
plt.grid(True)

plt.show()

```
</p>
</details>
