### 10줄 정리

딥러닝은 선형 회귀와 로지스틱 회귀로 나뉜다.
선형 회귀: 정보를 통해 최적의 그래프를 찾아내는 작업
"학생들의 중간고사 성적이 [ ]에 따라 다 다르다."에서 성적이 종속 변수이고, [ ]이 독립 변수이다.

최소제곱법은 주어진 x의 값이 하나일 때 적용 가능하다.
최소제곱법을 통해 일차 함수의 기울기 a와 y절편 b를 바로 구할 수 있다.
예측 직선은 오차가 최저가 되는 직선이다.
예측 값들을 이은 직선이어서 다른 x의 값을 집어넣어 y의 값을 예측할 수 있다.

평균제곱오차에서 입력 받는 데이터가 여러 종류라면 여러번 그려서 오차가 적은 쪽으로 수정해 나간다.
평균 제곱 오차를 가장 작게 만들어주는 최적의 a, b를 구하여 값을 예측한다.

경사 하강법은 기울기 a와 오차와의 관계를 이차 함수 그래프로 그린 것으로, 적절한 학습률을 설정해 미분 값이 0인 지점을 찾는다.
학습률은 적절한 이동거리를 정해주는 것으로, 단계 크기를 결정하는 최적화 알고리즘의 조정 매개변수이다.

다중 선형 회귀는 더 정확한 예측을 위해 사용한다.
